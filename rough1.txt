import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark import SparkConf, SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import
from pyspark.sql import functions as F
from pyspark.sql import SQLContext
from pyspark.sql.types import *
import pandas as pd
import numpy as np
import boto3
from botocore.config import Config
conn = boto.s3.connect_to_region('eu-west-1', calling_format=boto.s3.connection.OrdinaryCallingFormat())

my_config = Config(
    region_name = 'us-west-2',
    signature_version = 'v4',
    retries = {
        'max_attempts': 10,
        'mode': 'standard'
    }
)

sc = SparkContext("local", "Simple App")
spark = SQLContext(sc)
spark_conf = SparkConf().setMaster('local').setAppName('<YOUR_APP_NAME>')

SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
## @params: [JOB_NAME, URL, ACCOUNT, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'ACCOUNT', 'WAREHOUSE', 'DB', 'SCHEMA', 'USERNAME', 'PASSWORD'])

glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
java_import(spark._jvm, "SNOWFLAKE_SOURCE_NAME")

## uj = sc._jvm.net.snowflake.spark.snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : args['URL'],
"sfAccount" : args['ACCOUNT'],
"sfUser" : args['USERNAME'],
"sfPassword" : args['PASSWORD'],
"sfDatabase" : args['DB'],
"sfSchema" : args['SCHEMA'],
"sfWarehouse" : args['WAREHOUSE']

}
print("Success")
read_data = "(SELECT * FROM PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP ') as t"
#df=spark.read.format('jdbc').options(
         #url='pgedevdatalake.snowflakecomputing.com',
         #dbtable=read_data ,
         #user='olap',
         #password='K1ngK0ng#'
         #).load()
#df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP").load()
#query= ''' SELECT DISTINCT BANK_OBJECTID FROM PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP ''';

#bike= """select   BANK_OBJECTID from  PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP limit 10"""
#sql3="""select customer_name, sum(demand_amt) as total_spent from {0} GROUP BY  customer_name""".format(tbl1)
def load_query(xfmr):
    return f"""SELECT u.HOURLY_USAGE,
                   u.USAGE_DATE_BEGINNING,
                   u.USAGE_HOUR_BEGINNING,
                   h.BANK_OBJECTID,
                   h.LABELTEXT,
                   t.RATEDKVA,
                   (u.HOURLY_USAGE/ (CAST(t.RATEDKVA AS int))) AS ratio
            FROM (SELECT DISTINCT BANK_OBJECTID,
                                  LABELTEXT
                  FROM WAREHOUSE.HUB_GIS_ASSET_MAP
                  WHERE BANK_OBJECTID = {xfmr}) h,
                  GEOSPATIAL.TRANSFORMER t,
                  WAREHOUSE.TRANSFORMER_USAGE_HOURLY u
            WHERE h.LABELTEXT = t.LABELTEXT
                  AND u.XFMR_BANKID = h.BANK_OBJECTID
                  AND u.USAGE_DATE_BEGINNING >= '2018-06-01'
            ORDER BY u.USAGE_DATE_BEGINNING, u.USAGE_HOUR_BEGINNING
            """
def get_load(xfmr):
    print("the sam")
    load1 = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",load_query(xfmr)).load()
    a=load1.select([F.col(x).alias(x.lower()) for x in load1.columns])
    
    print(type(load1))
    b=a.toPandas()
    print(type(b))
    #load1.show()
    #load1['dttm'] = pd.to_datetime(load1.usage_date_beginning.astype(str) + ' ' + load1.usage_hour_beginning.astype(str) + ':00:00')
    b['time_interval'] = 1
    
    return b
def fahrenheit_to_celsius(temp):
    return (temp - 32)*5/9

### Calculate top oil and winding temperatures
def top_oil_temp(rise_at_rated_load, load_ratio, ratio_load_loss=2, n=0.8):
    return rise_at_rated_load*((load_ratio*ratio_load_loss + 1)/(ratio_load_loss + 1))**n

def winding_temp(rise_at_rated_load, load_ratio, m=0.8):
  return (rise_at_rated_load)*(load_ratio**((2*m)))

### Calculate rise over ambient temperature
def rise_over_ambient_temp(ultimate_temp, init_temp, time_constant, time_interval):
    return (ultimate_temp - init_temp)*(1 - np.exp(-time_interval/time_constant)) + init_temp

def calculate_temp(xfmr_part, load_ratio, time_interval, time_constant, rise_at_rated_load):
    if xfmr_part not in ['winding', 'topoil']:
        raise ValueError('xfmr_part must be one of "winding" or "topoil"')
    
    if xfmr_part == 'winding':
      ultimate_temp = winding_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)							 
    else:
        ultimate_temp = top_oil_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)
    
    temp_0 = load_ratio.iloc[0]
    init_temp = ultimate_temp.shift(1, fill_value=temp_0)
    
    return rise_over_ambient_temp(ultimate_temp=ultimate_temp,
                                  init_temp=init_temp,
                                  time_interval=time_interval,
                                  time_constant=time_constant)

### Get winding & top oil rise over ambient
def get_temps(load, capacity, time_interval, winding_time_constant=4/60, oil_time_constant=180/60, to_rise_at_rated_load=52.82, w_rise_at_rated_load=15.96):
    load_ratio = (load/capacity).astype('float')
    
    w_temp = calculate_temp(xfmr_part='winding',
                            load_ratio=load_ratio,
                            time_interval=time_interval,
                            time_constant=winding_time_constant,
                            rise_at_rated_load=w_rise_at_rated_load)

    to_temp = calculate_temp(xfmr_part='topoil',
                             load_ratio=load_ratio,
                             time_interval=time_interval,
                             time_constant=oil_time_constant,
                             rise_at_rated_load=to_rise_at_rated_load)
							 
    return w_temp, to_temp

### Get hotspot temperature
def hotspot_temp(ambient_temp, top_oil_temp, winding_temp):
    return ambient_temp + top_oil_temp + winding_temp

### Calculate aging metrics
def factor_accelerated_aging(hotspot_temp):
    return np.exp(15000/383 - 15000/(hotspot_temp + 273))

def factor_equivalent_aging(faa):
    return np.mean(faa)

def loss_of_life(feqa, life_span=180000):
    return feqa*24*100/life_span
    
def main(xfmr):
    print("Runget_loadning function for ",xfmr )
    #tym = datetime.now()
    # Query load data for a given 
    #load = get_load(xfmr=xfmr)
    df = get_load(xfmr=xfmr)
    #df.select([F.col(x).alias(x.lower()) for x in df.columns]).show()
    # Grab transformer capacity from the query
    print("raja")
    print(df)
    if df["bank_objectid"].count()>0:
        # Grab transformer capacity from the query
        capacity = df.ratedkva.iloc[0]
        # Calculate winding and top oil temperatures from load and capacity
        
        df['winding_temp'], df['top_oil_temp'] = get_temps(load=df.hourly_usage,
                                                           capacity=capacity,
                                                       time_interval=df.time_interval)
	
	    # Calculate hotspot temperature from ambient, winding, and top oil temperatures
        df['hotspot_temp'] = hotspot_temp(ambient_temp=df.mean_temp.astype('float'),
                                          winding_temp=df.winding_temp,
                                          top_oil_temp=df.top_oil_temp)
        
        # Calculate hourly accelerated aging from the hotspot temperature
        df['faa'] = factor_accelerated_aging(df.hotspot_temp)
        df['faa'] = np.where(df.ratio.abs() > 1,
                             df.faa,
                             0)    
        # Calculate a rolling 24 hour equivalent aging factor
        df['feqa'] = df.groupby('usage_date_beginning')['faa'].transform('mean')  
        
        # Calculate % loss of life
        df['lol'] = loss_of_life(df.feqa)
        print(df)
        return df
   
def get_bank_ids():
        sql= ''' SELECT DISTINCT BANK_OBJECTID FROM WAREHOUSE.HUB_GIS_ASSET_MAP limit 10'''
        results = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",sql).load()  
        #r=spark.select(lower(col('BANK_OBJECTID')).alias('bla'))
        results = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",sql).load()
        #result=results.select('BANK_OBJECTID').collect()
        mvv_list = results.selectExpr("BANK_OBJECTID as mvv")
        mvv_arr = [int(row['mvv']) for row in mvv_list.collect()]
        fin=[]
        for i in mvv_arr:
            fin.append(i)
        #print(fin)
        #for col in results.columns:
            #result = results.withColumn(col, F.lower(F.col(col)))
            #result=results.select(lower(col('BANK_OBJECTID')))
            #print(result)
            #return(result)
        return(fin)
#bank_id_df = get_bank_ids()
for i in get_bank_ids():
    print(i)
    main(i)
-------------------------------

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark import SparkConf, SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import
from datetime import datetime
from pyspark.sql import functions as F
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from json import dumps
from io import StringIO
import os
import pandas as pd
import numpy as np
import boto3

sc = SparkContext("local", "Simple App")
spark = SQLContext(sc)
spark_conf = SparkConf().setMaster('local').setAppName('<YOUR_APP_NAME>')

SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
## @params: [JOB_NAME, URL, ACCOUNT, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'ACCOUNT', 'WAREHOUSE', 'DB', 'SCHEMA', 'USERNAME', 'PASSWORD'])

glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
java_import(spark._jvm, "SNOWFLAKE_SOURCE_NAME")

## uj = sc._jvm.net.snowflake.spark.snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : args['URL'],
"sfAccount" : args['ACCOUNT'],
"sfUser" : args['USERNAME'],
"sfPassword" : args['PASSWORD'],
"sfDatabase" : args['DB'],
"sfSchema" : args['SCHEMA'],
"sfWarehouse" : args['WAREHOUSE']

}
print("Success")
read_data = "(SELECT * FROM PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP ') as t"
#df=spark.read.format('jdbc').options(
         #url='pgedevdatalake.snowflakecomputing.com',
         #dbtable=read_data ,
         #user='olap',
         #password='K1ngK0ng#'
         #).load()
#df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP").load()
#query= ''' SELECT DISTINCT BANK_OBJECTID FROM PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP ''';

#bike= """select   BANK_OBJECTID from  PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP limit 10"""
#sql3="""select customer_name, sum(demand_amt) as total_spent from {0} GROUP BY  customer_name""".format(tbl1)
### Query data
def load_query(xfmr):
    return f"""
                with t1 as (
                        SELECT u.HOURLY_USAGE,
                            u.USAGE_DATE_BEGINNING,
                            u.USAGE_HOUR_BEGINNING,
                            h.BANK_OBJECTID,
                            h.LABELTEXT,
                            t.RATEDKVA,
                            (u.HOURLY_USAGE+1/ (CAST(t.RATEDKVA AS int))) AS ratio,    
                            dateadd(hour, u.USAGE_HOUR_BEGINNING, u.USAGE_DATE_BEGINNING) as dttma,
                            current_timestamp as t
                        FROM (SELECT DISTINCT BANK_OBJECTID,LABELTEXT
                                FROM WAREHOUSE.HUB_GIS_ASSET_MAP
                                    WHERE BANK_OBJECTID in ({xfmr})
                               ) h,
                            GEOSPATIAL.HG_TRANSFORMER_3 t,
                            WAREHOUSE.TRANSFORMER_USAGE_HOURLY_1 u
                                WHERE h.LABELTEXT = t.LABELTEXT
                                AND u.XFMR_BANKID = h.BANK_OBJECTID
                                AND u.USAGE_DATE_BEGINNING >= '2018-01-05' 
                        ),
                t2 as 
                (
                    select dt as dttm , mean_temp from weather.HG_EXT_INPUT_WEATHER_HIST
                )
                select * from t1
                inner join t2 on t1.dttma = t2.dttm
            """

def get_load(xfmr):
  
    load1 = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",load_query(xfmr)).load()
    a=load1.select([F.col(x).alias(x.lower()) for x in load1.columns])
    load=a.toPandas()
    load['time_interval'] = 1    
    
    print(load)
    return load

def fahrenheit_to_celsius(temp):
    return (temp - 32)*5/9

### Calculate top oil and winding temperatures
def top_oil_temp(rise_at_rated_load, load_ratio, ratio_load_loss=2, n=0.8):
    return rise_at_rated_load*((load_ratio*ratio_load_loss + 1)/(ratio_load_loss + 1))**n

def winding_temp(rise_at_rated_load, load_ratio, m=0.8):
  return (rise_at_rated_load)*(load_ratio**((2*m)))

### Calculate rise over ambient temperature
def rise_over_ambient_temp(ultimate_temp, init_temp, time_constant, time_interval):
    return (ultimate_temp - init_temp)*(1 - np.exp(-time_interval/time_constant)) + init_temp

def calculate_temp(xfmr_part, load_ratio, time_interval, time_constant, rise_at_rated_load):
    if xfmr_part not in ['winding', 'topoil']:
        raise ValueError('xfmr_part must be one of "winding" or "topoil"')
    
    if xfmr_part == 'winding':
      ultimate_temp = winding_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)							 
    else:
        ultimate_temp = top_oil_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)
    
    temp_0 = load_ratio.iloc[0]
    init_temp = ultimate_temp.shift(1, fill_value=temp_0)
    
    return rise_over_ambient_temp(ultimate_temp=ultimate_temp,
                                  init_temp=init_temp,
                                  time_interval=time_interval,
                                  time_constant=time_constant)

### Get winding & top oil rise over ambient
def get_temps(load, capacity, time_interval, winding_time_constant=4/60, oil_time_constant=180/60, to_rise_at_rated_load=52.82, w_rise_at_rated_load=15.96):
    load_ratio = (load/capacity).astype('float')
    
    w_temp = calculate_temp(xfmr_part='winding',
                            load_ratio=load_ratio,
                            time_interval=time_interval,
                            time_constant=winding_time_constant,
                            rise_at_rated_load=w_rise_at_rated_load)

    to_temp = calculate_temp(xfmr_part='topoil',
                             load_ratio=load_ratio,
                             time_interval=time_interval,
                             time_constant=oil_time_constant,
                             rise_at_rated_load=to_rise_at_rated_load)
							 
    return w_temp, to_temp

### Get hotspot temperature
def hotspot_temp(ambient_temp, top_oil_temp, winding_temp):
    return ambient_temp + top_oil_temp + winding_temp

### Calculate aging metrics
def factor_accelerated_aging(hotspot_temp):
    return np.exp(15000/383 - 15000/(hotspot_temp + 273))

def factor_equivalent_aging(faa):
    return np.mean(faa)

def loss_of_life(feqa, life_span=180000):
    return feqa*24*100/life_span
    
def main(xfmr):
    
    
    
    tym = datetime.now()

    # Query load data for a given 
    df = get_load(xfmr)
    
    print("Time Taken by Query =", datetime.now() - tym , " seconds.")
    
    print("count= ",df["bank_objectid"].count())
    
    if df["bank_objectid"].count()>0:
        # Grab transformer capacity from the query
        capacity = df.ratedkva.iloc[0]
        
        # Calculate winding and top oil temperatures from load and capacity
        
        df['winding_temp'], df['top_oil_temp'] = get_temps(load=df.hourly_usage,
                                                           capacity=capacity,
                                                       time_interval=df.time_interval)
	
	    # Calculate hotspot temperature from ambient, winding, and top oil temperatures
        df['hotspot_temp'] = hotspot_temp(ambient_temp=df.mean_temp.astype('float'),
                                          winding_temp=df.winding_temp,
                                          top_oil_temp=df.top_oil_temp)
        
        # Calculate hourly accelerated aging from the hotspot temperature
        df['faa'] = factor_accelerated_aging(df.hotspot_temp)
        df['faa'] = np.where(df.ratio.abs() > 1,
                             df.faa,
                             0)    
        # Calculate a rolling 24 hour equivalent aging factor
        df['feqa'] = df.groupby('usage_date_beginning')['faa'].transform('mean')  
        
        # Calculate % loss of life
        df['lol'] = loss_of_life(df.feqa)
   
        print("Time Taken (Query + Other Calculations) =", datetime.now() - tym , " seconds.")
        print("/***********************************************************/")
        print("\n")
        print(df)
         ## write data to S3
		bucketName = 'dsm-datalake-s3-bucket-dev'
		conn = boto.s3.connect_to_region('eu-west-1', calling_format=boto.s3.connection.OrdinaryCallingFormat())
        bucket = conn.get_bucket(bucketName)
        bucketName = 'dsm-datalake-s3-bucket-dev'
        s3_prefix = "Datalake_Raw/Weather/overload_xfmr/mat"
        dt = datetime.now()
        datestr = dt.strftime("%Y%m%d")
        timestr = dt.strftime("%H%M%S%f")
        fileName = 't_overloaded_xfmr_'+datestr+'_'+timestr+'.csv'
        #fileName = 't_overloaded_xfmr_'+str(xfmr)+'.csv'
    
        file_prefix = "/".join([s3_prefix,fileName])

        csv_buffer = StringIO()
        df.to_csv(csv_buffer)
		#client = boto3.client('kinesis', config=my_config)
    
        s3_resource = boto3.resource('s3',config=my_config)
        #s3 = boto3.client('s3', verify=False)
        s3_resource.Object("bucketName", file_prefix).put(Body=csv_buffer.getvalue())
        return df
 
def get_bank_ids():
        sql= ''' SELECT DISTINCT BANK_OBJECTID FROM WAREHOUSE.HUB_GIS_ASSET_MAP limit 100'''
        results = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",sql).load()
        #result=results.select('BANK_OBJECTID').collect()
        mvv_list = results.selectExpr("BANK_OBJECTID as mvv")
        mvv_arr = [int(row['mvv']) for row in mvv_list.collect()]
        fin=[]
        for i in mvv_arr:
            fin.append(i)
        return(fin)
#bank_id_df = get_bank_ids()
for i in get_bank_ids():
    print(i)
    main(i)
  *********Glue working model *******


import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark import SparkConf, SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import
from datetime import datetime
from pyspark.sql import functions as F
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from json import dumps
from io import StringIO
import snowflake.connector 
from snowflake.sqlalchemy import URL
from sqlachemy import create_engine
import os
import pandas as pd
import numpy as np
import boto3


from botocore.config import Config

my_config = Config(
    region_name = 'us-west-2',
    signature_version = 'v4',
    retries = {
        'max_attempts': 10,
        'mode': 'standard'
    }
)
sc = SparkContext("local", "Simple App")
spark = SQLContext(sc)
spark_conf = SparkConf().setMaster('local').setAppName('<YOUR_APP_NAME>')

SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
## @params: [JOB_NAME, URL, ACCOUNT, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'ACCOUNT', 'WAREHOUSE', 'DB', 'SCHEMA','USERNAME', 'PASSWORD','CONF'])

glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
java_import(spark._jvm, "SNOWFLAKE_SOURCE_NAME")

## uj = sc._jvm.net.snowflake.spark.snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : args['URL'],
"sfAccount" : args['ACCOUNT'],
"sfUser" : args['USERNAME'],
"sfPassword" : args['PASSWORD'],
"sfDatabase" : args['DB'],
"sfSchema" : args['SCHEMA'],
"sfWarehouse" : args['WAREHOUSE'],
"sfConf":args['CONF']

}
print("Success")
create_engine=sfOptions
engine=create_engine
connection=engine.connect()
read_data = "(SELECT * FROM PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP  ') as t"
#df=spark.read.format('jdbc').options(
         #url='pgedevdatalake.snowflakecomputing.com',
         #dbtable=read_data ,
         #user='olap',
         #password='K1ngK0ng#'
         #).load()
#df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP").load()
#query= ''' SELECT DISTINCT BANK_OBJECTID FROM PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP ''';

#bike= """select   BANK_OBJECTID from  PGEDWDEV.WAREHOUSE.HUB_GIS_ASSET_MAP limit 10"""
#sql3="""select customer_name, sum(demand_amt) as total_spent from {0} GROUP BY  customer_name""".format(tbl1)
### Query data
def load_query():
    return f"""select * from staging.overloaded_test limit 10"""

def get_load():
  
    load1 = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",load_query()).load()
    a=load1.select([F.col(x).alias(x.lower()) for x in load1.columns])
    load=a.toPandas()
    load['time_interval'] = 1    
    
    #print(load)
    return load

def fahrenheit_to_celsius(temp):
    return (temp - 32)*5/9

### Calculate top oil and winding temperatures
def top_oil_temp(rise_at_rated_load, load_ratio, ratio_load_loss=2, n=0.8):
    return rise_at_rated_load*((load_ratio*ratio_load_loss + 1)/(ratio_load_loss + 1))**n

def winding_temp(rise_at_rated_load, load_ratio, m=0.8):
  return (rise_at_rated_load)*(load_ratio**((2*m)))

### Calculate rise over ambient temperature
def rise_over_ambient_temp(ultimate_temp, init_temp, time_constant, time_interval):
    return (ultimate_temp - init_temp)*(1 - np.exp(-time_interval/time_constant)) + init_temp

def calculate_temp(xfmr_part, load_ratio, time_interval, time_constant, rise_at_rated_load):
    if xfmr_part not in ['winding', 'topoil']:
        raise ValueError('xfmr_part must be one of "winding" or "topoil"')
    
    if xfmr_part == 'winding':
      ultimate_temp = winding_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)							 
    else:
        ultimate_temp = top_oil_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)
    
    temp_0 = load_ratio.iloc[0]
    init_temp = ultimate_temp.shift(1, fill_value=temp_0)
    
    return rise_over_ambient_temp(ultimate_temp=ultimate_temp,
                                  init_temp=init_temp,
                                  time_interval=time_interval,
                                  time_constant=time_constant)

### Get winding & top oil rise over ambient
def get_temps(load, capacity, time_interval, winding_time_constant=4/60, oil_time_constant=180/60, to_rise_at_rated_load=52.82, w_rise_at_rated_load=15.96):
    load_ratio = (load/capacity).astype('float')
    
    w_temp = calculate_temp(xfmr_part='winding',
                            load_ratio=load_ratio,
                            time_interval=time_interval,
                            time_constant=winding_time_constant,
                            rise_at_rated_load=w_rise_at_rated_load)

    to_temp = calculate_temp(xfmr_part='topoil',
                             load_ratio=load_ratio,
                             time_interval=time_interval,
                             time_constant=oil_time_constant,
                             rise_at_rated_load=to_rise_at_rated_load)
							 
    return w_temp, to_temp

### Get hotspot temperature
def hotspot_temp(ambient_temp, top_oil_temp, winding_temp):
    return ambient_temp + top_oil_temp + winding_temp

### Calculate aging metrics
def factor_accelerated_aging(hotspot_temp):
    return np.exp(15000/383 - 15000/(hotspot_temp + 273))

def factor_equivalent_aging(faa):
    return np.mean(faa)

def loss_of_life(feqa, life_span=180000):
    return feqa*24*100/life_span
    
def main():
    
    
    
    tym = datetime.now()

    # Query load data for a given 
    df = get_load()
    
   # print("Time Taken by Query =", datetime.now() - tym , " seconds.")
    
#    print("count= ",df["bank_objectid"].count())
    
    if df["bank_objectid"].count()>0:
        # Grab transformer capacity from the query
        capacity = df.ratedkva.iloc[0]
        
        # Calculate winding and top oil temperatures from load and capacity
        
        df['winding_temp'], df['top_oil_temp'] = get_temps(load=df.hourly_usage,
                                                           capacity=capacity,
                                                       time_interval=df.time_interval)
	
	    # Calculate hotspot temperature from ambient, winding, and top oil temperatures
        df['hotspot_temp'] = hotspot_temp(ambient_temp=df.mean_temp.astype('float'),
                                          winding_temp=df.winding_temp,
                                          top_oil_temp=df.top_oil_temp)
        
        # Calculate hourly accelerated aging from the hotspot temperature
        df['faa'] = factor_accelerated_aging(df.hotspot_temp)
        df['faa'] = np.where(df.ratio.abs() > 1,
                             df.faa,
                             0)    
        # Calculate a rolling 24 hour equivalent aging factor
        df['feqa'] = df.groupby('usage_date_beginning')['faa'].transform('mean')  
        
        # Calculate % loss of life
        df['lol'] = loss_of_life(df.feqa)
        df.to_sql('my_table',con=engine,index=False)
        print("table created")
   
        #print(df)
        #df_sp = spark.createDataFrame(df)
        #df
        #print(type(df_sp)
         ## write data to S3
        bucketName = "dsm-datalake-s3-bucket-dev"
        s3_prefix = "Datalake_Raw/test1"
        dt = datetime.now()
        datestr = dt.strftime("%Y%m%d")
        timestr = dt.strftime("%H%M%S%f")
        fileName = 'overloaded_xfmr_'+datestr+'_'+timestr+'.csv'

        file_prefix = "/".join([s3_prefix,fileName])

        csv_buffer = StringIO()
        df.to_csv(csv_buffer)
        ssm = boto3.client('ssm',region_name='us-west-2')
        kmskey=ssm.get_parameter(Name='/datalake/storage/s3/kmskey', WithDecryption=True)['Parameter']['Value']
        s3_resource = boto3.resource('s3')
        s3_resource.Object(bucketName, file_prefix).put(Body=csv_buffer.getvalue() ,ServerSideEncryption= "aws:kms", SSEKMSKeyId =kmskey)
        print("Done!")
main()  
*****************************
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark import SparkConf, SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import
from datetime import datetime
from pyspark.sql import functions as F
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from json import dumps
from io import StringIO
import os
import pandas as pd
import numpy as np
import boto3


from botocore.config import Config
sc = SparkContext("local", "Simple App")
spark = SQLContext(sc)
spark_conf = SparkConf().setMaster('local').setAppName('<YOUR_APP_NAME>')
SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
## @params: [JOB_NAME, URL, ACCOUNT, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'ACCOUNT', 'WAREHOUSE', 'DB', 'SCHEMA','USERNAME', 'PASSWORD','CONF'])
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
java_import(spark._jvm, "SNOWFLAKE_SOURCE_NAME")
## uj = sc._jvm.net.snowflake.spark.snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : args['URL'],
"sfAccount" : args['ACCOUNT'],
"sfUser" : args['USERNAME'],
"sfPassword" : args['PASSWORD'],
"sfDatabase" : args['DB'],
"sfSchema" : args['SCHEMA'],
"sfWarehouse" : args['WAREHOUSE'],
"sfConf":args['CONF']

}
print("Success")
read_data = "select * from staging.overloaded_test limit 10"
a = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",read_data).load()
b=a.select([F.col(x).alias(x.lower()) for x in a.columns])
df=b.toPandas()
df['time_interval'] = 1
def fahrenheit_to_celsius(temp):
    return (temp - 32)*5/9

### Calculate top oil and winding temperatures
def top_oil_temp(rise_at_rated_load, load_ratio, ratio_load_loss=2, n=0.8):
    return rise_at_rated_load*((load_ratio*ratio_load_loss + 1)/(ratio_load_loss + 1))**n

def winding_temp(rise_at_rated_load, load_ratio, m=0.8):
  return (rise_at_rated_load)*(load_ratio**((2*m)))

### Calculate rise over ambient temperature
def rise_over_ambient_temp(ultimate_temp, init_temp, time_constant, time_interval):
    return (ultimate_temp - init_temp)*(1 - np.exp(-time_interval/time_constant)) + init_temp

def calculate_temp(xfmr_part, load_ratio, time_interval, time_constant, rise_at_rated_load):
    if xfmr_part not in ['winding', 'topoil']:
        raise ValueError('xfmr_part must be one of "winding" or "topoil"')
    
    if xfmr_part == 'winding':
      ultimate_temp = winding_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)							 
    else:
        ultimate_temp = top_oil_temp(rise_at_rated_load=rise_at_rated_load,
                                     load_ratio=load_ratio)
    
    temp_0 = load_ratio.iloc[0]
    init_temp = ultimate_temp.shift(1, fill_value=temp_0)
    
    return rise_over_ambient_temp(ultimate_temp=ultimate_temp,
                                  init_temp=init_temp,
                                  time_interval=time_interval,
                                  time_constant=time_constant)

### Get winding & top oil rise over ambient
def get_temps(load, capacity, time_interval, winding_time_constant=4/60, oil_time_constant=180/60, to_rise_at_rated_load=52.82, w_rise_at_rated_load=15.96):
    load_ratio = (load/capacity).astype('float')
    
    w_temp = calculate_temp(xfmr_part='winding',
                            load_ratio=load_ratio,
                            time_interval=time_interval,
                            time_constant=winding_time_constant,
                            rise_at_rated_load=w_rise_at_rated_load)

    to_temp = calculate_temp(xfmr_part='topoil',
                             load_ratio=load_ratio,
                             time_interval=time_interval,
                             time_constant=oil_time_constant,
                             rise_at_rated_load=to_rise_at_rated_load)
							 
    return w_temp, to_temp

### Get hotspot temperature
def hotspot_temp(ambient_temp, top_oil_temp, winding_temp):
    return ambient_temp + top_oil_temp + winding_temp

### Calculate aging metrics
def factor_accelerated_aging(hotspot_temp):
    return np.exp(15000/383 - 15000/(hotspot_temp + 273))

def factor_equivalent_aging(faa):
    return np.mean(faa)

def loss_of_life(feqa, life_span=180000):
    return feqa*24*100/life_span
    
capacity = df.ratedkva.iloc[0]
        
        # Calculate winding and top oil temperatures from load and capacity
        
df['winding_temp'], df['top_oil_temp'] = get_temps(load=df.hourly_usage,
                                                           capacity=capacity,
                                                       time_interval=df.time_interval)
	
	    # Calculate hotspot temperature from ambient, winding, and top oil temperatures
df['hotspot_temp'] = hotspot_temp(ambient_temp=df.mean_temp.astype('float'),
                                          winding_temp=df.winding_temp,
                                          top_oil_temp=df.top_oil_temp)
        
        # Calculate hourly accelerated aging from the hotspot temperature
df['faa'] = factor_accelerated_aging(df.hotspot_temp)
df['faa'] = np.where(df.ratio.abs() > 1,
                             df.faa,
                             0)    
        # Calculate a rolling 24 hour equivalent aging factor
df['feqa'] = df.groupby('usage_date_beginning')['faa'].transform('mean')  
        
        # Calculate % loss of life
df['lol'] = loss_of_life(df.feqa)
        
df1 = spark.createDataFrame(df)
df1.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "rajatest").mode('overwrite')
print("done")
#spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",read_data).load()
**************************rekha job*****


import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import

SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"

## @params: [JOB_NAME, URL, ACCOUNT, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'ACCOUNT', 'WAREHOUSE', 'DB', 'SCHEMA', 'USERNAME', 'PASSWORD'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
java_import(spark._jvm, SNOWFLAKE_SOURCE_NAME)



## uj = sc._jvm.net.snowflake.spark.snowflake, enable query pushdown to Snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : args['URL'],
"sfAccount" : args['ACCOUNT'],
"sfUser" : args['USERNAME'],
"sfPassword" : args['PASSWORD'],
"sfDatabase" : args['DB'],
"sfSchema" : args['SCHEMA'],
"sfWarehouse" : args['WAREHOUSE'],
}

print("Test")


## Read from a Snowflake table into a Spark Data Frame
df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "PGEDWDEV.WAREHOUSE.HUB_HR_EMPLOYEE").load()
df.show()


## Perform any kind of transformations on your data and save as a new Data Frame: "df1"
## df1 = df.[Insert any filter, transformation, or other operation]
## Write the Data Frame contents back to Snowflake in a new table 
## df1.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "[new_table_name]").mode("overwrite").save() 
job.commit()

----------------------
working code
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark import SparkConf, SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from py4j.java_gateway import java_import
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number
from datetime import datetime
from pyspark.sql import functions as F
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql import SQLContext
from pyspark.sql.types import *
from pyspark.sql.functions import *
from json import dumps
from io import StringIO
import os
import pandas as pd
import numpy as np
from datetime import datetime
from dateutil import tz
import boto3
from pyspark.sql.functions import from_utc_timestamp

#sc = SparkContext("local", "Simple App")
#conf = SparkConf.set("spark.executor.memory", "16g")
#sc =  SparkContext(conf)

spark_con = (
    SparkConf()
    .setAppName("Your App Name")
    .set('spark.executor.memory', '10g')
    .set('spark.driver.memory', '19g')
    .set('spark.yarn.executor.memoryOverhead','5g')
    .set('spark.driver.maxResultSize','0')
    .set("spark.memory.fraction", '0.8')
    .set("spark.sql.shuffle.partitions" , '100')
   #.set("spark.sql.execution.arrow.pyspark.enabled", "true")
    #.set("spark.sql.execution.arrow.enabled", "true")
    .set('spark.sql.autoBroadcastJoinThreshold', '-1')
    .set('spark.sql.session.timeZone', 'UTC'))
#conf = SparkConf.setAll([('spark.executor.memory', '23g'), ('spark.driver.memory','9g')])
#sc = SparkContext(conf)
#conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '23g'), ('spark.driver.memory','9g')])
#sc =  SparkContext(conf)
#spark = SQLContext(sc)
sc = SparkContext(conf = spark_con)
#sc = SparkContext(conf = spark_con, serializer=ArrowSerializer())
spark = SQLContext(sc)
spark_conf = SparkConf().setMaster('local').setAppName('<YOUR_APP_NAME>')
SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
## @params: [JOB_NAME, URL, ACCOUNT, WAREHOUSE, DB, SCHEMA, USERNAME, PASSWORD]
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'URL', 'ACCOUNT', 'WAREHOUSE', 'DB', 'SCHEMA','USERNAME', 'PASSWORD'])
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
java_import(spark._jvm, "SNOWFLAKE_SOURCE_NAME")
## uj = sc._jvm.net.snowflake.spark.snowflake
spark._jvm.net.snowflake.spark.snowflake.SnowflakeConnectorUtils.enablePushdownSession(spark._jvm.org.apache.spark.sql.SparkSession.builder().getOrCreate())
sfOptions = {
"sfURL" : args['URL'],
"sfAccount" : args['ACCOUNT'],
"sfUser" : args['USERNAME'],
"sfPassword" : args['PASSWORD'],
"sfDatabase" : args['DB'],
"sfSchema" : args['SCHEMA'],
"sfWarehouse" : args['WAREHOUSE'],
#"sfConf":args['CONF']
}
#conn= snowflake.connector.connect(user=con_target['user'],password=con_target['password'],account=con_target['account'],database='PGEDWDEV')
print("Success")
# Define constants
winding_time_constant = 4/60
oil_time_constant = 180/60
to_rise_at_rated_load = 52.82
w_rise_at_rated_load = 15.96
ratio_load_loss = 2
n = 0.8
m = 0.8
time_interval = 1
life_span=180000

# Read Data
sql = "select * from staging.overloaded_test"
df = spark.read.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("query",sql).load()

# Clean column names
df = df.select([F.col(x).alias(x.lower()) for x in df.columns])
#df.show(2,False)
#df.show()
# Create initial columns
df = df.withColumn('dte', F.date_trunc('day', df['dttm']))
df=df.withColumn('mean_temp', (df['mean_temp'] -32)*5/9)
df=df.withColumn('w_ult_temp', (to_rise_at_rated_load)*(df['ratio']**((2*m))))
df=df.withColumn('to_ult_temp', (to_rise_at_rated_load)*((df['ratio']*ratio_load_loss + 1)/(ratio_load_loss + 1))**n)
#df.show(2,False)
# Create lag window
#lw = Window.partitionBy([F.col('bank_objectid')).orderBy(F.col('dttm')])
#lw = Window.partitionBy([F.col('bank_objectid')).orderBy(F.col('dttm')])
lw=Window.partitionBy("bank_objectid").orderBy("dttm")
# Create lag columns
df=df.withColumn('w_init_temp', F.lag(df['w_ult_temp']).over(lw)) 
df=df.withColumn('to_init_temp', F.lag(df['to_ult_temp']).over(lw)) 
df=df.withColumn('w_rise_temp', (df['w_ult_temp'] - df['w_init_temp'])*(1 -np.exp(-time_interval/winding_time_constant)) + df['w_init_temp']) 
df=df.withColumn('to_rise_temp', (df['to_ult_temp'] - df['to_init_temp'])*(1 - np.exp(-time_interval/oil_time_constant)) + df['to_init_temp'])
df=df.withColumn('hotspot_temp', df['mean_temp'] + df['w_rise_temp'] + df['to_rise_temp'])
df=df.withColumn('accelerated_aging', F.exp(15000/383 - 15000/(df['hotspot_temp'] + 273)))
# Create partition window for factor of equivalent aging
fw = Window.partitionBy([F.col('bank_objectid'), F.col('dte')])

# Create factor of equivalent aging
df =df.withColumn('feqa', F.mean(df['accelerated_aging']).over(fw)) 
df=df.withColumn('loss_of_life', df['feqa']*24*100/life_span)

#df.show(2,False)
df.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "rajatest11").mode('overwrite').save()
print("done")
--------------------------

NOAAA -COMPLETED---------------

import json
import time
import pytz
from pytz import timezone
import requests
import datetime
from datetime import timedelta
from dateutil.tz import tzutc, UTC
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
from pandas import DataFrame
import numpy as np
from io import BytesIO
from json import dumps
from datetime import datetime as dt

import boto3
s3 = boto3.client('s3')
s3 = boto3.resource("s3")
ssm = boto3.client('ssm', region_name='us-west-2')
kmskey = ssm.get_parameter(Name='/datalake/storage/s3/kmskey', WithDecryption=True)['Parameter']['Value']
noaa_codes = [
    'KAST',
    'KBDN',
    'KCVO',
    'KEUG',
    'KHIO',
    'KHRI',
    'KMMV',
    'KONP',
    'KPDX',
    'KRDM',
    'KSLE',
    'KSPB',
    'KTMK',
    'KTTD',
    'KUAO'
]
urls = [f"https://api.weather.gov/stations/{x}/observations/latest" for x in noaa_codes]
file = [line.strip() for line in urls]
ssm = boto3.client('ssm',region_name='us-west-2')
accountid=ssm.get_parameter(Name='/datalake/bi_accountid', WithDecryption=True)['Parameter']['Value']

print("accountid ", accountid)

if accountid == "475210740017":
    print("Dev Environment")
    s3_bucket1= "dsm-datalake-s3-bucket-dev"
    s3_bucket = "dsm-datascience-dev"
    s3_bucket3= "test-par"
elif accountid == "679494167814":
    print("Test Environment")
    s3_bucket1= "dsm-datalake-s3-bucket-test"
    s3_bucket = "dsm-datascience-test"
    s3_bucket3= "test-par"
elif accountid == "554931768202":
    print("Prod Environment")
    s3_bucket1= "dsm-datalake-s3-bucket-prod"
    s3_bucket = "dsm-datascience-prod"
    #s3_bucket3= "test-par"

s3_prefix1 = "Datalake_Raw/Weather/Noaa/measurement/PARQ_FILES"
s3_prefix = "Datalake_Raw/Weather/Noaa/measurement"
#s3_prefix2 = "Datalake_Raw/Weather/Noaa/measurement/LATEST_24HRS"
filepath = "Datalake_Raw/Weather/Noaa/measurement/Latest_48Hours"
remove_filepath="Datalake_Raw/Weather/Noaa/measurement/Latest_48Hours/noaa"
#bucketName = 'dsm-datalake-s3-bucket-dev'
timeRange  = 48

def get_datetime():
    dt = datetime.datetime.now()
    return dt.strftime("%Y%m%d"), dt.strftime("%H:%M:%S")
    
def remove_files(s3_bucket1,remove_filepath,timeRange):
  s3 = boto3.resource('s3')
  bucket = s3.Bucket(s3_bucket1)
  files=[]
  for object in bucket.objects.filter(Prefix=remove_filepath):
    if (object.last_modified <= datetime.datetime.now(tzutc()) - timedelta(hours = timeRange)) :   
      files.append(object.key)
      response = bucket.delete_objects(Delete={'Objects': [{'Key': object.key }]})
  return files
  
def reshape(r):
    cor=r["geometry"]
    props = r["properties"]
    res = {
        "X_COORDINATES":cor["coordinates"][0],
        "Y_COORDINATES":cor["coordinates"][1],
        "STATION": props["station"].split("/")[-1],
        "txd":props["textDescription"],
        "CELSIUS_TEMP": props["temperature"]["value"],
        "DEWPOINT": props["dewpoint"]["value"],
        "SEALEVELPRESSURE": props["seaLevelPressure"]["value"],
        "BAROMETRICPRESSURE": props["barometricPressure"]["value"],
        "PRCPLH":props["precipitationLastHour"]["value"],
        "VISIBILITY": props["visibility"]["value"],
        "WINDSPEED": props["windSpeed"]["value"],
        "WINDGUST": props["windGust"]["value"],
        "HUMIDITY": props["relativeHumidity"]["value"],
        "WINDDIRECTION": props["windDirection"]["value"]
    }
    return res

    
def lambda_handler(event, context):
    """
    """
    outputStatus = 0 ## Success=1 , Failed=0
    errorMessage = ''
    responses = []
    try:
        for url in file:
            r = requests.get(url)
            while r.status_code == 404:
                print("The URL is not hit")
                time.sleep(300)
            if r.status_code != 404:
                print("the URl is HIT ")
            responses.append(reshape(r.json()))
            
        s3_prefix1 = "Datalake_Raw/Weather/Noaa/measurement/PARQ_FILES"
        s3_prefix = "Datalake_Raw/Weather/Noaa/measurement"
        datestr,timestr = get_datetime()
        fname = f"noaa_hourly_measurements_latest"
        file_prefix = "/".join([s3_prefix,fname])
        fname1 = f"noaa_hourly_measurements.PARQUET_FILE_{datestr}_{timestr}"
        fname2 = f"noaa_hourly_measurements.JSON_FILE"
        file_prefix1 = "/".join([s3_prefix1,fname1])
        file_prefix3 = "/".join([filepath,fname1])
        file_prefix2 = "/".join([s3_prefix,fname2])
        s3_obj = s3.Object(s3_bucket, file_prefix)
        serialized = []
        for r in responses:
            serialized.append(json.dumps(r))
        jsonlines_doc = "\n".join(serialized)
        #s3_obj.put(Body=json.dumps(jsonlines_doc))
        df = pd.read_json(jsonlines_doc, lines=True)
        df['FER_TEMP']= ((df['CELSIUS_TEMP'] * 9/5) + 32).round(2)
        df['DEWPOINT_FER']=((df['DEWPOINT'] * 9/5) +32).round(2)
        df['SEALEVELPRESSURE']=df['SEALEVELPRESSURE'].div(1000).round(2)
        df['PRCPLH']=df['PRCPLH'].fillna(0)
        #df['PRCPLH']=df['PRCPLH'].astype(int)
        df.insert(0, 'timestamp', pd.datetime.now().replace(microsecond=0))
        date = pd.datetime.now(tz=pytz.utc)
        date1 = date.astimezone(timezone('US/Pacific'))
        dateformat=date1.strftime("%Y-%m-%d %H:%M:%S")
        df.insert(0, 'TIMESTAMP_MEASUREMENT_PST',dateformat)
        #df['CURRENT_PRCP'] = df['PRCPLH']
        df['CURRENT_SNDP'] = df['txd'].str.contains('Snow').astype(int)
        df['CURRENT_FOG'] = df['txd'].str.contains('Fog').astype(int)
        df['CURRENT_RAIN_DRIZZLE'] = df['txd'].str.contains('Rain_Drizzle').astype(int)
        df['CURRENT_SNOW_ICE_PELLLETS'] = df['txd'].str.contains('Ice').astype(int)
        df['CURRENT_HAIL'] = df['txd'].str.contains('Hail').astype(int)
        df['CURRENT_THUNDER'] = df['txd'].str.contains('Thunder').astype(int)
        df['CURRENT_TORANDO_FUNNEL_CLOUD'] = df['txd'].str.contains('Torando').astype(int)
        df.insert(0, 'TIMESTAMP_MEASUREMENT_DEFAULT', datetime.datetime.now().replace(microsecond=0))
        df.drop('timestamp',axis='columns', inplace=True)
        for col in df.columns:
            print(col)
        print(df)
        print(df['txd'])
        print(df['PRCPLH'])
        print(df['FER_TEMP'])
        print(df['SEALEVELPRESSURE'])
        
        out_buffer = BytesIO()
        df.to_parquet(out_buffer)
        s3_resource = boto3.resource('s3')
        #s3_resource.Object(s3_bucket3, file_prefix1).put(Body=out_buffer.getvalue())
        s3_resource.Object(s3_bucket1, file_prefix1).put(Body=out_buffer.getvalue())
        s3_resource.Object(s3_bucket1, file_prefix3).put(Body=out_buffer.getvalue())
        print("parquet file Written ")
        #json_buffer = BytesIO()
        df1=df.to_json(orient='records')
        s3_obj = s3.Object(s3_bucket, file_prefix)
        s3_obj.put(Body=json.dumps(df1))
        print("json file written ")
        files = remove_files(s3_bucket1,remove_filepath,timeRange)
        print("**********Task Completed******")
        outputStatus = 1 ## Success=1 , Failed=0
        errorMessage = 'No Error'
    except :
        outputStatus = 0 ## Success=1 , Failed=0
        errorMessage = "Error"        
    return {
        'Status'        :   outputStatus,
        'errorMessage'   :  errorMessage
        }
    

    #ExtraArgs={"ServerSideEncryption": "aws:kms", "SSEKMSKeyId": kmskey})
    #s3_resource.object(filename, bucketname, objectkey, ExtraArgs={"ServerSideEncryption": "aws:kms", "SSEKMSKeyId": })
    #s3_resource.upload_file(filename, bucketname, objectkey, ExtraArgs={"ServerSideEncryption": "aws:kms", "SSEKMSKeyId": })import json
	
	--------------------------------------
	
import re
import pandas as pd
import numpy as np
from census import Census
import census_tables_dict as d
import json
import datetime
from io import BytesIO
from datetime import timedelta
#from datetime import datetime as dt
import boto3
s3_resource = boto3.resource('s3')

def get_datetime():
    dt = datetime.datetime.now()
    #dt = datetime.now()
    return dt.strftime("%Y%m%d"), dt.strftime("%H:%M:%S")
    

# Whether to return wide data (~830 rows, 340 columns) or long (~280k rows, 6 columns)
long = False

# FIXME: insert AWS account Census API key
# https://api.census.gov/data/key_signup.html
c = Census('29cb02926405d359ad2d480254dca1e2d11dfcb8')


def get_census_data(census_obj, long=long):
    # Define lists
    census_id = list()
    census_vars = list()
    human_readable = list()

    # Create mapping between Census tables and human-readable column names
    for i in d.tables.keys():
        cen_name = d.tables[i]['census_id']
        census_id.append(cen_name)

        census_vars = census_vars + \
                        [cen_name + '_' + variable + 'E' for variable in d.tables[i]['variables']] + \
                        [cen_name + '_' + variable + 'M' for variable in d.tables[i]['variables']]
        human_readable = human_readable + \
                        [name + 'E' for name in d.tables[i]['var_names']] + \
                        [name + 'M' for name in d.tables[i]['var_names']]

    # Request from Census API
    census_tup = tuple(['NAME'] + census_vars)
    census_df = pd.DataFrame(census_obj.acs5.state_county_tract(census_tup, '41', Census.ALL, Census.ALL))
	
	# Ensure column order
    census_df = census_df[['NAME', 'state', 'county', 'tract'] + census_vars]

    # Rename columns with human-readable names
    census_df.columns = ['name', 'state', 'county', 'tract'] + human_readable
    
    # Ensure ID columns are strings
    for i in ['state', 'county', 'tract']:
        census_df[i] = census_df[i].astype(int).astype(str)

    # Reformat to long structure, if desired
    if long:
        census_df = pd.melt(census_df,
                            id_vars=['name', 'state', 'county', 'tract'])

    # Create GEOID column to link to the service point mapping table
    census_df['GEOID'] = census_df.state + \
                            census_df.county.str.pad(width=3,
                                                              fillchar='0') + \
                            census_df.tract.str.pad(width=6,
                                                             fillchar='0')
    census_df['year'] = datetime.datetime.now().year - 2                
	
    #print(census_df)
    return census_df

def transform_census_data(census):
    
    # Industry columns
    industry_cols = [i for i in census.columns if ('industry_' in i) and ('E' in i)]
    
    census['industry_total'] = census[industry_cols].sum(axis=1)
    
    for i in industry_cols:
        census['perc_' + i] = census[i]/census['industry_total']
    
    # Computing devices columns
    census['perc_hh_has_computing_devices'] = census.hh_has_computing_devicesE/(census.hh_has_computing_devicesE + census.hh_has_no_computerE)
    census['perc_hh_has_no_computing_devices'] = census.hh_has_no_computerE/(census.hh_has_computing_devicesE + census.hh_has_no_computerE)
    
    # Education columns
    educ_cols = [i for i in census.columns if ('educ_' in i) and ('E' in i)]
    census['educ_total'] = census[educ_cols].sum(axis=1)
    for i in educ_cols:
        census['perc_' + i] = census[i]/census['educ_total']
    
    # Labor force columns
    census['perc_pop_not_in_labor_force'] = census.pop_not_in_labor_forceE/census.pop_work_ageE
    census['perc_pop_in_civ_labor_force_unemp'] = census.pop_in_civ_labor_force_unempE/census.pop_in_civ_labor_forceE
    
    # Household type columns
    census['perc_hh_type_family_married'] = census.hh_type_family_marriedE/census.hh_typeE
    census['perc_hh_type_single_parent'] = (census.hh_type_family_other_single_dadE + census.hh_type_family_other_single_momE)/census.hh_typeE
    census['perc_hh_type_nonfamily_roommates'] = census.hh_type_nonfamily_roommatesE/census.hh_typeE
    census['perc_hh_type_nonfamily_bach'] = census.hh_type_nonfamily_bachE/census.hh_typeE
    
    # Rent cost columns
    rent_cols = ['hh_rent_10_perc_income',
                 'hh_rent_10to15_perc_income',
                 'hh_rent_15to20_perc_income',
                 'hh_rent_20to25_perc_income',
                 'hh_rent_25to30_perc_income',
                 'hh_rent_30to35_perc_income',
                 'hh_rent_35to40_perc_income',
                 'hh_rent_40to45_perc_income',
                 'hh_rent_45to50_perc_income',
                 'hh_rent_over50_perc_income']

    for i in rent_cols:
        census['perc_' + i] = census[i + 'E']/census.hh_rentE
        
    # Health insurance columns
    census['perc_health_ins_private'] = (census.num_per_native_health_ins_privateE + census.num_per_foreign_natur_health_ins_privateE + census.num_per_foreign_noncit_health_ins_privateE)/census.num_perE
    census['perc_health_ins_private'] = (census.num_per_native_health_ins_publicE + census.num_per_foreign_natur_health_ins_publicE + census.num_per_foreign_noncit_health_ins_publicE)/census.num_perE
    census['perc_health_ins_private'] = (census.num_per_native_no_health_insE + census.num_per_foreign_natur_no_health_insE + census.num_per_foreign_noncit_no_health_insE)/census.num_perE
    census['perc_native'] = census.num_per_nativeE/census.num_perE
    census['perc_foreign_natur'] = census.num_per_foreign_naturE/census.num_perE
    census['perc_foreign_noncit'] = census.num_per_foreign_noncitE/census.num_perE
    
    # Work transportation columns
    census['perc_work_transp_car'] = census.work_transp_carE/census.work_transpE
    census['perc_work_transp_public'] = census.work_transp_publicE/census.work_transpE
    
    # Number of household workers columns
    worker_cols = [i for i in census.columns if ('hh_workers_' in i) and ('E' in i)]
    census['hh_workers_total'] = census[worker_cols].sum(axis=1)
    for i in worker_cols:
        census['perc_' + i] = census[i]/census['hh_workers_total']
    
    # Occupation columns
    occup_cols = [i for i in census.columns if ('occup_' in i) and ('E' in i)]
    for i in occup_cols:
        census['perc' + i] = census[i]/census.occupE
    
    # Poverty level
    census['perc_poverty_lev_below'] = census.poverty_lev_belowE/census.poverty_levE
    
    # Received welfare
    census['perc_hh_welfare_received'] = census.hh_welfare_receivedE/census.hh_welfareE
    
    # Internet columns
    census['hh_internet_subscrip'] = census.hh_internet_subscripE/census.hh_internetE
    census['hh_internet_no_subscrip'] = census.hh_internet_no_subscripE/census.hh_internetE
    census['hh_internet_no_access'] = census.hh_internet_no_accessE/census.hh_internetE
    
    # Race columns
    race_cols = [i for i in census.columns if ('hh_race_' in i) and ('E' in i)]
    for i in race_cols:
        census['perc' + i] = census[i]/census.hh_raceE
    
    # Ratio of income to poverty level columns
    inc_to_pov_cols = [i for i in census.columns if ('hh_inc_to_pov_' in i) and ('E' in i)]
    census['hh_inc_to_pov_lev_total'] = census[inc_to_pov_cols].sum(axis=1)
    for i in inc_to_pov_cols:
        census['perc_' + i] = census[i]/census['hh_inc_to_pov_lev_total']
    
    # Adjust columns
    cen_cols = [i for i in census.columns if (('perc_' in i) and not ('_perc_' in i)) or (('E' in i) and (('median' in i) or ('gini_' in i) or ('avg' in i)))]
    all_cols = ['name', 'state', 'county', 'tract', 'GEOID', 'year']
    all_cols.extend(cen_cols)
    
    census = census[all_cols]
    
    census.columns = [re.sub('E$', '', i) for i in census.columns]
    
    
    return census

s3_bucket="dsm-datalake-s3-bucket-test"
s3_prefix = "Datalake_Raw/Census/Census_acs_5yr"
s3_prefix1 = "Datalake_Raw/Census/Census_acs_features"
datestr,timestr = get_datetime()
fname = f"CENSUS_ACS_5YR_{datestr}_{timestr}"
fname1 = f"CENSUS_ACS_FEATURES_{datestr}_{timestr}"
file_prefix = "/".join([s3_prefix,fname])
file_prefix1 = "/".join([s3_prefix1,fname1])




def lambda_handler(event, context):
    df=get_census_data(c)
    df1=transform_census_data(df)
    out_buffer = BytesIO()
    out_buffer1 = BytesIO()
    df.to_parquet(out_buffer)
    df1.to_parquet(out_buffer1)
    s3_resource.Object(s3_bucket, file_prefix).put(Body=out_buffer.getvalue())
    s3_resource.Object(s3_bucket, file_prefix1).put(Body=out_buffer1.getvalue())
    
    print("parquet files Written ")
    
    
    # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Success!')
    }
----------------------

ERRORNOTIFICATIONARN = '/datalake/sns/error_noaa_test'
SUCCESSSNSARNGRIDACCOUNT='/datalake/extract_noaa/success_test'
COMPONENT_NAME = 'Datalake_LAMBDA_NOAA_EXTRACT'
ERROR_MSG = 'URL IS NOT HIT*** PLEASE RERUN AFTER SOMETIME***'
SUCCESS_MSG = 'SUCCESSFULLY EXTRACTED NOAA FILES FOR 14 STATIONS'
SUCCESS_DESCRIPTION='SUCCESS'
ERROR_TYPE='API ERROR'
SOURCE_URL='/datalake/extract_noaa/api'


#ERRORNOTIFICATIONARN = '/datalake/sns/errornotificationarn'
#SUCCESSSNSARNGRIDACCOUNT='/adms/extract_avl/success_sns_arn_grid_account'
#COMPONENT_NAME = 'Datalake-Extract-NOAA Lambda'
#ERROR_MSG = 'URL IS NOT HIT***PLEASE RERUN AFTER SOMETIME****'
-------------------------------------------------------------------

import re
import pandas as pd
import numpy as np
from census import Census
import census_tables_dict as d
import constants as constant
import json
import datetime
from io import BytesIO
from datetime import timedelta
# from datetime import datetime as dt
import boto3

s3_resource = boto3.resource('s3')


def get_datetime():
    dt = datetime.datetime.now()
    # dt = datetime.now()
    return dt.strftime("%Y%m%d"), dt.strftime("%H:%M:%S")


# Whether to return wide data (~830 rows, 340 columns) or long (~280k rows, 6 columns)
long = False

# FIXME: insert AWS account Census API key
# https://api.census.gov/data/key_signup.html
c = Census('a4642d28de30944a2e2c6c60bdc2f62559bcb14e')

###function for Error  SNS Notification#####
def send_error_sns(error_description, error_type, error_grouping_type, error_message):
    ssm = boto3.client("ssm", region_name="us-west-2")
    sns = boto3.client("sns", region_name="us-west-2")
    error_sns_arn = ssm.get_parameter(Name=constant.ERRORNOTIFICATIONARN)["Parameter"]["Value"]
    env = ssm.get_parameter(Name=constant.ENVIRONMENT, WithDecryption=True)['Parameter']['Value']
    component_name = constant.COMPONENT_NAME
    sns_message = (f"{component_name} : {error_description} : {error_type} : {error_grouping_type} : {error_message}")
    err_response = sns.publish(
        TargetArn=error_sns_arn,
        Message=json.dumps({'default': json.dumps(sns_message)}),
        Subject=env + " : " + component_name,
        MessageStructure="json",
        MessageAttributes={
            "job_name": {"DataType": "String.Array",
                         "StringValue": 'Datalake-Extract-NOAA'}})
    return err_response

###function for Success notification####
def send_sns_success():
    sns = boto3.client("sns", region_name="us-west-2")
    ssm = boto3.client("ssm", region_name="us-west-2")
    success_sns_arn = ssm.get_parameter(Name=constant.SUCCESSNOTIFICATIONARN, WithDecryption=True)["Parameter"]["Value"]
    component_name = constant.COMPONENT_NAME
    env = ssm.get_parameter(Name=constant.ENVIRONMENT, WithDecryption=True)['Parameter']['Value']
    success_msg = constant.SUCCESS_MSG
    sns_message = (f"{component_name} :  {success_msg}")
    print(sns_message, 'text')
    succ_response = sns.publish(
        TargetArn=success_sns_arn,
        Message=json.dumps({'default': json.dumps(sns_message)}),
        Subject=env + " : " + component_name,
        MessageStructure="json",
        MessageAttributes={
            "job_name": {
                "DataType": "String.Array",
                "StringValue": '["DATALAKE-Extract-CENSUS"]'}})
    

    return succ_response

#Function for getting  the census data from census api via generated key ###
def get_census_data(census_obj, long=long):
    # Define lists
    census_id = list()
    census_vars = list()
    human_readable = list()

    # Create mapping between Census tables and human-readable column names
    for i in d.tables.keys():
        cen_name = d.tables[i]['census_id']
        census_id.append(cen_name)

        census_vars = census_vars + \
                      [cen_name + '_' + variable + 'E' for variable in d.tables[i]['variables']] + \
                      [cen_name + '_' + variable + 'M' for variable in d.tables[i]['variables']]
        human_readable = human_readable + \
                         [name + 'E' for name in d.tables[i]['var_names']] + \
                         [name + 'M' for name in d.tables[i]['var_names']]

    # Request from Census API
    census_tup = tuple(['NAME'] + census_vars)
    census_df = pd.DataFrame(census_obj.acs5.state_county_tract(census_tup, '41', Census.ALL, Census.ALL))
    # print("key activated")

    # except APIKeyError:
    # send_error_sns()

    # Ensure column order
    census_df = census_df[['NAME', 'state', 'county', 'tract'] + census_vars]

    # Rename columns with human-readable names
    census_df.columns = ['name', 'state', 'county', 'tract'] + human_readable

    # Ensure ID columns are strings
    for i in ['state', 'county', 'tract']:
        census_df[i] = census_df[i].astype(int).astype(str)

    # Reformat to long structure, if desired
    if long:
        census_df = pd.melt(census_df,
                            id_vars=['name', 'state', 'county', 'tract'])

    # Create GEOID column to link to the service point mapping table
    census_df['GEOID'] = census_df.state + \
                         census_df.county.str.pad(width=3,
                                                  fillchar='0') + \
                         census_df.tract.str.pad(width=6,
                                                 fillchar='0')
    census_df['year'] = datetime.datetime.now().year - 2

    # print(census_df)
    return census_df

##function for generating new cols####
def transform_census_data(census):
    try:

        # Industry columns
        industry_cols = [i for i in census.columns if ('industry_' in i) and ('E' in i)]
        print(industry_cols, 'gotham')

        census['industry_total'] = census[industry_cols].sum(axis=1)

        for i in industry_cols:
            census['perc_' + i] = census[i] / census['industry_total']

        # Computing devices columns
        census['perc_hh_has_computing_devices'] = census.hh_has_computing_devicesE / (
                census.hh_has_computing_devicesE + census.hh_has_no_computerE)
        census['perc_hh_has_no_computing_devices'] = census.hh_has_no_computerE / (
                census.hh_has_computing_devicesE + census.hh_has_no_computerE)

        # Education columns
        educ_cols = [i for i in census.columns if ('educ_' in i) and ('E' in i)]
        census['educ_total'] = census[educ_cols].sum(axis=1)
        for i in educ_cols:
            census['perc_' + i] = census[i] / census['educ_total']

        # Labor force columns
        census['perc_pop_not_in_labor_force'] = census.pop_not_in_labor_forceE / census.pop_work_ageE
        census[
            'perc_pop_in_civ_labor_force_unemp'] = census.pop_in_civ_labor_force_unempE / census.pop_in_civ_labor_forceE

        # Household type columns
        census['perc_hh_type_family_married'] = census.hh_type_family_marriedE / census.hh_typeE
        census['perc_hh_type_single_parent'] = (
                                                       census.hh_type_family_other_single_dadE + census.hh_type_family_other_single_momE) / census.hh_typeE
        census['perc_hh_type_nonfamily_roommates'] = census.hh_type_nonfamily_roommatesE / census.hh_typeE
        census['perc_hh_type_nonfamily_bach'] = census.hh_type_nonfamily_bachE / census.hh_typeE

        # Rent cost columns
        rent_cols = ['hh_rent_10_perc_income',
                     'hh_rent_10to15_perc_income',
                     'hh_rent_15to20_perc_income',
                     'hh_rent_20to25_perc_income',
                     'hh_rent_25to30_perc_income',
                     'hh_rent_30to35_perc_income',
                     'hh_rent_35to40_perc_income',
                     'hh_rent_40to45_perc_income',
                     'hh_rent_45to50_perc_income',
                     'hh_rent_over50_perc_income']

        for i in rent_cols:
            census['perc_' + i] = census[i + 'E'] / census.hh_rentE

        # Health insurance columns
        census['perc_health_ins_private'] = (
                                                    census.num_per_native_health_ins_privateE + census.num_per_foreign_natur_health_ins_privateE + census.num_per_foreign_noncit_health_ins_privateE) / census.num_perE
        census['perc_health_ins_private'] = (
                                                    census.num_per_native_health_ins_publicE + census.num_per_foreign_natur_health_ins_publicE + census.num_per_foreign_noncit_health_ins_publicE) / census.num_perE
        census['perc_health_ins_private'] = (
                                                    census.num_per_native_no_health_insE + census.num_per_foreign_natur_no_health_insE + census.num_per_foreign_noncit_no_health_insE) / census.num_perE
        census['perc_native'] = census.num_per_nativeE / census.num_perE
        census['perc_foreign_natur'] = census.num_per_foreign_naturE / census.num_perE
        census['perc_foreign_noncit'] = census.num_per_foreign_noncitE / census.num_perE

        # Work transportation columns
        census['perc_work_transp_car'] = census.work_transp_carE / census.work_transpE
        census['perc_work_transp_public'] = census.work_transp_publicE / census.work_transpE

        # Number of household workers columns
        worker_cols = [i for i in census.columns if ('hh_workers_' in i) and ('E' in i)]
        census['hh_workers_total'] = census[worker_cols].sum(axis=1)
        for i in worker_cols:
            census['perc_' + i] = census[i] / census['hh_workers_total']

        # Occupation columns
        occup_cols = [i for i in census.columns if ('occup_' in i) and ('E' in i)]
        for i in occup_cols:
            census['perc' + i] = census[i] / census.occupE

        # Poverty level
        census['perc_poverty_lev_below'] = census.poverty_lev_belowE / census.poverty_levE

        # Received welfare
        census['perc_hh_welfare_received'] = census.hh_welfare_receivedE / census.hh_welfareE

        # Internet columns
        census['hh_internet_subscrip'] = census.hh_internet_subscripE / census.hh_internetE
        census['hh_internet_no_subscrip'] = census.hh_internet_no_subscripE / census.hh_internetE
        census['hh_internet_no_access'] = census.hh_internet_no_accessE / census.hh_internetE

        # Race columns
        race_cols = [i for i in census.columns if ('hh_race_' in i) and ('E' in i)]
        for i in race_cols:
            census['perc' + i] = census[i] / census.hh_raceE

        # Ratio of income to poverty level columns
        inc_to_pov_cols = [i for i in census.columns if ('hh_inc_to_pov_' in i) and ('E' in i)]
        census['hh_inc_to_pov_lev_total'] = census[inc_to_pov_cols].sum(axis=1)
        for i in inc_to_pov_cols:
            census['perc_' + i] = census[i] / census['hh_inc_to_pov_lev_total']

        # Adjust columns
        cen_cols = [i for i in census.columns if (('perc_' in i) and not ('_perc_' in i)) or (
                ('E' in i) and (('median' in i) or ('gini_' in i) or ('avg' in i)))]
        all_cols = ['name', 'state', 'county', 'tract', 'GEOID', 'year']
        all_cols.extend(cen_cols)
        census = census[all_cols]
        census.columns = [re.sub('E$', '', i) for i in census.columns]
    ####SUCCESS-SNS-NOTIFICATION ACTIVATED#####    
        send_sns_success()
        return census
    ####ERROR -SNS NOTIFICATION ACTIVATED#####
    except IndexError as index_error:
        send_error_sns("UNABLE_TRANF_FILE", "TECHNICAL_ERROR", "FILE_PROC_ERRORS", error_message=str(index_error))
    except Exception as e:
         send_error_sns("SYSTEM_ERRORS", "TECHNICAL_ERROR", "LAMBDA_ERRORS", error_message=str(e))
         
###BUCKETDETAILS FOR WRITING PARQUET FILES###
s3_bucket = "dsm-datalake-s3-bucket-dev"
s3_prefix = "Datalake_Raw/Census/Census_acs_5yr"
s3_prefix1 = "Datalake_Raw/Census/Census_acs_features"
datestr, timestr = get_datetime()
fname = f"CENSUS_ACS_5YR_{datestr}_{timestr}"
fname1 = f"CENSUS_ACS_FEATURES_{datestr}_{timestr}"
file_prefix = "/".join([s3_prefix, fname])
file_prefix1 = "/".join([s3_prefix1, fname1])


def lambda_handler(event, context):
    ####API ERROR EXCEPTION HANDLING####
    try:
        d = get_census_data(c)
    except Exception as e:
        Error_message=constant.ERROR_MSG
        send_error_sns("API KEY_ERRORS", "AUTH_ERROR", "API_ERRORS", error_message=Error_message)
    df1 = transform_census_data(d)
    # df1=df1.fillna(value=np.nan)
    print(type(df1))
    out_buffer = BytesIO()
    out_buffer1 = BytesIO()
    d.to_parquet(out_buffer)
    df1.to_parquet(out_buffer1)
    s3_resource.Object(s3_bucket, file_prefix).put(Body=out_buffer.getvalue())
    s3_resource.Object(s3_bucket, file_prefix1).put(Body=out_buffer1.getvalue())
    print("parquet files Written ")
        # TODO implement
    return {
        'statusCode': 200,
        'body': json.dumps('Success!')
    }
-------------------------

import datetime 
def get_datetime():
    dt1 = datetime.datetime.now()
    return dt1.strftime("%d %B, %Y")
monthstr = get_datetime()
ERRORNOTIFICATIONARN = '/datalake/sns/errornotificationarn'
SUCCESSNOTIFICATIONARN='/datalake/sns/successnotificationarn'
COMPONENT_NAME = 'DL_LAMBDA_CENSUS_EXTRACT'
ERROR_MSG = f'NEED ATTENTION ****API ERROR /KEY EXPIRED ** ON {monthstr} ******'
SUCCESS_MSG = f'SUCCESSFULLY EXTRACTED CENSUS FILES FOR {monthstr}***'
SUCCESS_DESCRIPTION='SUCCESS'
ENVIRONMENT = '/matillion/environment'
------------------------------

kinesis

#import json
#from __future__ import print_function
#import base64

import sys
import logging
import psycopg2
import boto3
import json
import random
import calendar
import time
from datetime import datetime
#from psycopg2.extras import LogicalReplicationConnection


#def lambda_handler(event, context):
    # TODO implement
#    return {
#        'statusCode': 200,
#        'body': json.dumps('Hello from Lambda!')
#    }


#def lambda_handler(event, context):
#    for record in event['Records']:
#       #Kinesis data is base64 encoded so decode here
#       payload=base64.b64decode(record["kinesis"]["data"])
#       print("Decoded payload: " + str(payload))
       
kinesis_client = boto3.client('kinesis', region_name='us-east-2')

#my_connection  = psycopg2.connect("dbname='postgres' host='oms-dev-pge.ctkndbmfsp2d.us-west-2.rds.amazonaws.com' user='omspostgres' password='omspostgres'" )

con_source='dbname=postgres user=omspostgres password=omspostgres host=oms-dev-pge.ctkndbmfsp2d.us-west-2.rds.amazonaws.com port=5432'
my_connection=psycopg2.connect(con_source)

def lambda_handler(event, context):
    cur = my_connection.cursor()
    #cur.create_replication_slot('wal2json_oms_slot', output_plugin = 'wal2json')
    #cur.start_replication(slot_name = 'wal2json_oms_slot', options = {'pretty-print' : 1}, decode= True)
    #cur.consume_stream(consume)
    
def consume(msg):
    kinesis_client.put_record(StreamName=datalake-outage-notification, Data=json.dumps(msg.payload), PartitionKey="default")
    print (msg.payload)
#######

cursor = context.cursor()
cursor.execute("select table_name \
from information_schema.tables \
where table_type = 'BASE TABLE' and table_schema='WAREHOUSE'")
table_list=cursor.fetchall()
table_list=[str(''.join(x)) for x in table_list]
#table_list=['hub_cust_service_point_test']
for tablename in table_list:
  if not('DL' in tablename or 'BKP' in tablename or 'ETL' in tablename or 'CRAFT' in tablename):
    print tablename
    try:
      cursor.execute("alter table warehouse.{} add column ETL_CREATE_TIMESTAMP TIMESTAMP_NTZ(9)".format(tablename))
      cursor.execute("alter table warehouse.{} add column ETL_UPDATE_TIMESTAMP TIMESTAMP_NTZ(9)".format(tablename))
      cursor.execute("alter table warehouse.{} add column LOADED_BY VARCHAR(100)".format(tablename))
    except:
      print 'Updated Already'
      continue


  


######################

CREATE OR REPLACE  PIPE LANDINGTEMP.PIPE_PHISHING_EMAIL_STG
AUTO_INGEST=TRUE 
AS
COPY INTO
LANDINGTEMP.PHISHING_EMAIL_STG
FROM
(SELECT
'I',
CURRENT_TIMESTAMP(), 
$1,
$2,
$3,
$4,
$5,
TO_TIMESTAMP($6,'YYYY-MM-DD HH24:MI:SS UTC') AS DATE_EMAIL_OPENED,
$7,
TO_TIMESTAMP($8,'YYYY-MM-DD HH24:MI:SS UTC') AS DATE_CLICKED,
$9,
$10,
$11,
$12,
$13,
$14,
$15,
$16,
$17,
$18,
$19,
$20,
$21,
$22,
$23,
$24,
$25,
$26,
$27,
$28,
$29,
$30,
$31,
$32,
$33,
$34,
$35,
$36,
$37,
$38,
$39,
$40,
$41,
$42,
$43,
$44,
$45,
$46,
$47,
$48,
$49,
$50,
$51,
$52,
$53,
$54,
$55,
$56,
$57,
$58,
$59,
$60,
$61,
$62,
$63,
$64,
$65,
$66,
$67,
$68,
$69,
$70,
$71,
$72,
$73,
$74,
$75
FROM  @PUBLIC.DMS_S3_SOURCE_DATA_ENC/Datalake_Raw/Cyber-Proofpoint/
)
FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='"' 
FIELD_DELIMITER = "," SKIP_HEADER=1);

################3

TASK 

REATE OR REPLACE TASK LANDINGTEMP.TASK_PHISHING_EMAIL
	WAREHOUSE=DATA_INGESTION
	SCHEDULE='USING CRON 0 0 * * * America/Los_Angeles'
	AS CALL STAGING.PROC_MERGE_CDC('CYBER','PHISHING_EMAIL','LANDINGTEMP','PHISHING_EMAIL_STG',
	   'PHISHING_EMAIL_SWAP','EMAIL_ADDRESS,CAMPAIGN_TITLE,DATE_SENT','Y',CURRENT_DATABASE(),'PHISHING_EMAIL_SEQ');

ALTER TASK LANDINGTEMP.TASK_PHISHING_EMAIL RESUME;
-------------



df 1 = date dimesions + other columns 
df2 = df1/weeknumber 
Customer = df1 +df4

sales date,  quantity, product id 
                               iphone 
df1 = spark.sql ( select sales date , procudct id from table name group by producti) 
a=[100,5,82,....]
for i in a:
id a[i]=5 :
print (yes)

--------------
mongodb
from datetime import datetime
import pymongo
myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb = myclient["guid"]
mycol = mydb["guid_repo"]
mycol1=mydb["golden_repo"]
query = {"GUID": '4567'}
result = mycol1.delete_many(query)
result = mycol.delete_many(query)
query = {"GUID": '1234'}
result = mycol1.delete_many(query)
result = mycol.delete_many(query)

mylist1 = [
{"GUID":"1234","domain":"person","Source":"Siebel","Primary_Key":"S123","Fname":"John","Lname":"Doe","address": [{"address_type": "Primary","address_line_1": "4D Ag Lavras st","city": "Kifissia","postal_code": "14561","country": "Greece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":["123-456-7809","234-456-6789"],"Email":[{"type":"Primary","emailadd":"John.doe@abc.com"},{"type":"alternate","emailadd":["j.doe@abc.com","john.d@xyz.com"]}],"degree":"PHD","isMember":"Y","okToCall":"N","Publication":3,"Consolidation_Ind":4},
{"GUID":"1234","domain":"person","Source":"Author","Primary_Key":"A123","Fname":"J","Lname":"Doe","address": [{"address_type": "Primary","address_line_1": "4D Ag Lavras st","city": "Kifissia","postal_code": "14561","country": "Greece"},{"address_type": "Work","address_line_1": "xyz Ag Lavras st","city": "Madurai","postal_code": "141561","country": "India"}],"Phone":"123-456-7809","Email":[{"type":"Primary","emailadd":"j.doe@abc.com"},{"type":"alternate","emailadd":"john.d@author.com"}],"degree":["BS","MS"],"okToCall":"N","Publication":1,"Consolidation_Ind":4},
{"GUID":"1234","domain":"person","Source":"Marketto","Primary_Key":"M123","Fname":"John","Lname":"Doe","address": [{"address_type": "Primary","address_line_1": "4D Ag Lavras st","city": "Kifissia","postal_code": "14561","country": "Greece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":["342-876-9087","234-456-6789"],"Email":[{"type":"Primary","emailadd":"John.doe@abc.com"}],"isMember":"N","okToCall":"Y","Consolidation_Ind":4},
{"GUID":"1234","domain":"person","Source":"Reviewer","Primary_Key":"R123","Fname":"Jon","Lname":"Doe","address": [{"address_type": "Primary","address_line_1": "4D Ag Lavras st","city": "Kifissia","postal_code": "14561","country": "Greece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":"234-456-6789","Email":[{"type":"Primary","emailadd":"John.doe@abc.com"},{"type":"alternate","emailadd":"jon.Doe123@test.com"}],"degree":"MS","isMember":"N","Publication":1,"Consolidation_Ind":4},
{"GUID":"4567","domain":"person","Source":"Author","Primary_Key":"A56789","Fname":"Peter","Lname":"Parker","address": [{"address_type": "Primary","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":["309-309-3098","409-409-4890"],"Email":[{"type":"Primary","emailadd":"peter.parker@spider.com"},{"type":"alternate","emailadd":["p.parker@azas.com","peter.p@xyz.com"]}],"degree":"Btech","isMember":"","okToCall":"Y","Consolidation_Ind":4},
{"GUID":"4567","domain":"person","Source":"Marketto","Primary_Key":"A56789","Fname":"Peter S","Lname":"Parker","address": [{"address_type": "Primary","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":"409-409-4890","Email":[{"type":"Primary","emailadd":"p.parker@azas.com"}],"degree":"Mtech","isMember":"N","Publication":2,"Consolidation_Ind":4},
{"GUID":"4567","domain":"person","Source":"IEEE Apps","Primary_Key":"IA3456","Fname":"P","Lname":"Park","address": [{"address_type": "Primary","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":["309-309-3098","1800-call-spiderman"],"Email":[{"type":"Primary","emailadd":"peter.parker@spider.com"},{"type":"alternate","emailadd":"peter.p@xyz.com"}],"Consolidation_Ind":4},
{"GUID":"4567","domain":"person","Source":"CEEvents","Primary_Key":"CE5678909","Fname":"Pete","Lname":"Park","address": [{"address_type": "Primary","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":["345-0987-1234","890-090-0987"],"Email":[{"type":"Primary","emailadd":"p.parker@azas.com"},{"type":"alternate","emailadd":"pete.p@abcxyz.com"}],"degree":"Mtech","isMember":"","okToCall":"Y","Publication":1,"Consolidation_Ind":4},
{"GUID":"4567","domain":"person","Source":"OpenWater","Primary_Key":"OW56789","Fname":"Petey","Lname":"Parke","address": [{"address_type": "Primary","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"},{"address_type": "Work","address_line_1": "4DD Ag Lavras st","city": "KKifissia","postal_code": "141561","country": "GGreece"}],"Phone":"309-309-3098","dateofBirth":"08/01/1962","Email":[{"type":"alternate","emailadd":"stan.lee@comic.com"}],"isMember":"","okToCall":"Y","organization":"Marvel","Consolidation_Ind":4}]
# x = mycol.insert_many(mylist1)
# da = datetime.now().strftime("%Y-%m-%d_%I:%M:%S_%p")
# mycol.update_many({},{"$set": { "lastUpdated" :da }})
#mycol.update_many({},{"$set": { "oldguid" :'' }})
mylist2=[{'GUID': '1234', 'source_name': 'temp_golden', 'Fname': 'John', 'Lname': 'Doe', 'Email': [{'type': 'Primary', 'emailadd': ['John.doe@abc.com', 'j.doe@abc.com']}, {'type': 'alternate', 'emailadd': ['jon.Doe123@test.com', 'john.d@xyz.com', 'j.doe@abc.com', 'john.d@author.com']}], 'Phone': ['123-456-7809', '234-456-6789', '342-876-9087'], 'address': [{'address_type': 'Work', 'address_line_1': 'xyz Ag Lavras st', 'city': 'Madurai', 'postal_code': '141561', 'country': 'India'}, {'address_type': 'Work', 'address_line_1': '4DD Ag Lavras st', 'city': 'KKifissia', 'postal_code': '141561', 'country': 'GGreece'}, {'address_type': 'Primary', 'address_line_1': '4D Ag Lavras st', 'city': 'Kifissia', 'postal_code': '14561', 'country': 'Greece'}], 'isMember': 'Y', 'okToCall': 'N', 'Publication': 5}, {'GUID': '4567', 'source_name': 'temp_golden', 'Fname': 'Peter S', 'Lname': 'Parker', 'Email': [{'type': 'Primary', 'emailadd': ['p.parker@azas.com', 'peter.parker@spider.com']}, {'type': 'alternate', 'emailadd': ['peter.p@xyz.com', 'stan.lee@comic.com', 'p.parker@azas.com', 'pete.p@abcxyz.com']}], 'Phone': ['1800-call-spiderman', '309-309-3098', '345-0987-1234', '409-409-4890', '890-090-0987'], 'address': [{'address_type': 'Primary', 'address_line_1': '4DD Ag Lavras st', 'city': 'KKifissia', 'postal_code': '141561', 'country': 'GGreece'}, {'address_type': 'Work', 'address_line_1': '4DD Ag Lavras st', 'city': 'KKifissia', 'postal_code': '141561', 'country': 'GGreece'}], 'isMember': 'N', 'okToCall': 'Y', 'Publication': 3}]
#y =  mycol1.insert_many(mylist2)
#mycol1.update_many({},{"$set": { "unmerge" :'' }})



----------

import pymongo
from pymongo import MongoClient, UpdateOne, UpdateMany
from pprint import pprint
import pandas as pd
from pandas import DataFrame
import json
import numpy as np
from datetime import datetime
#import Consonants as cs
myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb = myclient["guid"]
mydb2 = myclient["Person"]
mycol2 = mydb2["Domain_Master"]
mycol = mydb["golden_repo"]
mycol1=mydb["guid_repo"]
cur=mycol1.find()
list_cur=list(cur)
#Before unmerge operation
guid_df=pd.DataFrame(list_cur)
print(guid_df)
# getting the input file from the collection ##

def getfilepath():
    a=mycol2.find({},{'domain_id':1,'IBUCKETPATH':1})
    path=[]
    for data in a:
        c=((data['IBUCKETPATH']))
        path.append(str(c))
        print(type(path))
        path1=' '.join(map(str, path))
        print(type(path1))
        return path1
a=getfilepath()
def inputid(a):
    data=open(a,'r')
    input_df = pd.read_csv(data)
    print(input_df)
    a = input_df['GUID'].tolist()
    print(a)
    golden_df = pd.DataFrame(list_cur)
    print(golden_df)
    b = golden_df['GUID'].tolist()
    print(b)
    c = [int(x) for x in b]
    #Comparing two lists#
    merge_guid = []
    for i in a:
        if i in c:
            print('Matched records with GR which needs to be unmerged', (i))
            merge_guid.append(str(i))
        else:
            print('Records which doesnot match', (i))
    print(type(merge_guid))
    print(merge_guid)
    return merge_guid


# #Calling the input function with file as a parameter
# #file = "D:/Users/input/guid_unmerge.txt"
res=inputid(a)
# #
# #
class unmerge:
    def __init__(self,id):
        self.id=id

    #Function for adding the flag field  for the GUIDS unmerge
    def unmerge_golden(self):
        for item in self.id:
            filter, update = {"GUID": item}, {"$set": {"unmerge": "yes"}}
            mycol.update_many(filter, update)
            print("Unmerged Record {} Updated in Mongodb".format(item))

# Function for removing  the  indicator fields and updating them with respect to the unmergedGUID
    def unmerge_remove_ind_guid(self):
        # col_name = 'Consolidation_Ind'
        for item in self.id:
            filter, update = {"GUID": item}, {'$unset': {'Consolidation_Ind': " "}}
            # db.testcollection.update_many({}, {"$unset": {f"{col_name}": 1}})
            mycol1.update_many(filter, update)
            # { $unset: {name: "", weight: ""}}
            print("Removed the consolidationindicatorof {} and Updated in Mongodb".format(item))

# Function for adding the OLD GUID fields and updating unmerged GUIDS
    def unmerge_guidrepo_update_oldguid(self):
        for item in self.id:
            filter, update = {"GUID": item}, {"$set": {"OLDGUID": item}}
            mycol1.update_many(filter, update)
            print("Updated oldguid {} in Mongodb".format(item))
# Function for emptying the  GUID fields and updating them with respect to the unmergedGUID
    def unmerge_empty_guid(self):
        for item in self.id:
            filter, update = {"GUID": item}, {"$set": {"GUID": " "}}
            mycol1.update_many(filter, update)
            print("Removed the oldguid {} and Updated in Mongodb".format(item))
def main():
    a = unmerge(inputid(getfilepath()))
    a.unmerge_golden()
    a.unmerge_remove_ind_guid()
    a.unmerge_guidrepo_update_oldguid()
    a.unmerge_empty_guid()
main()

---------------

import Updated_Golden_Unmerge as gu
import pymongo
from pymongo import MongoClient, UpdateOne, UpdateMany
import pandas as pd
import Consonants as cs
myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb2 = myclient["Person"]
mycol2 = mydb2["Domain_Master"]
mydb = myclient["guid"]
mycol = mydb["golden_repo"]
mycol1=mydb["guid_repo"]
cur=mycol1.find()
list_cur=list(cur)
#Writing the file in json after unmerge operation
df=pd.DataFrame(list_cur)
print(df)
def getfilepathfrwrite():
    a=mycol2.find({},{'domain_id':1,'WBUCKETPATH':1})
    ## domain_id which will be obtained
    path=[]
    for data in a:
        c=((data['WBUCKETPATH']))
        path.append(str(c))
        print(type(path))
        path1=' '.join(map(str, path))
        print(type(path1))
        return path1
writebucketpath= getfilepathfrwrite()

def writejson(writebucketpath):
    df = pd.DataFrame(list_cur)
    df.to_json(writebucketpath,default_handler=str, orient = 'records')
    print('files written')
    return df
#Calling the function
writejson(writebucketpath)

# Function for removing the UnMerged records in the Golden Repository
class remove_unmerge:
    def __init__(self,id):
        self.id=id
        #self.file=file
    def unmerge_delete_guid(self):
        for item in self.id:
          query = {"OLDGUID": item}
          result = mycol1.delete_many(query)
          print("records {} deleted".format(item))
          print("docs deleted:", result.deleted_count)
          #print("Removed the UNMERGED GUID {}  in GOLDEN REPO COLLECTION".format(item))


def unmerge_delete_guids():
    #file = "D:/Users/input/guid_unmerge.txt"
    b =remove_unmerge(gu.res)
    b.unmerge_delete_guid()
#Calling the  function
unmerge_delete_guids()
------------------
 import pyspark
 from pyspark import SparkContext
 from pyspark.sql import SQLContext
 from pyspark import SparkFiles
 from pyspark.sql import SparkSession
 from pyspark.sql.functions import *
 from pyspark.sql.functions import when
 from pyspark.sql.types import *
 import pyspark.sql.functions as F

 sc =SparkContext()
 spark = SparkSession.builder.getOrCreate()
  nums= sc.parallelize([1,2,3,4])
  squared = nums.map(lambda x: x*x).collect()
  print(squared)
  for num in squared:
      print('%i ' % (num))

 
 url = "https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv"
 sc.addFile(url)
 sqlContext = SQLContext(sc)
 df = sqlContext.read.csv(SparkFiles.get("adult_data.csv"), header=True, inferSchema= True)
 df = spark.read.csv(SparkFiles.get("adult_data.csv"), header=True, inferSchema= True)
 filename= 'C:/Users/10638056/Desktop/RRR Training/data/sparkdata.txt'
 df = spark.read.csv(filename, header=True, inferSchema= True)
 df_string = sqlContext.read.csv(SparkFiles.get("adult_data.csv"), header=True, inferSchema= False)
 df.printSchema()
 df.show(5, truncate = False)
 df_string = spark.read.csv(filename, header=True, inferSchema=  False)

df_string.printSchema()
 Write a custom function to convert the data type of DataFrame columns
def convertColumn(df, names, newType):
    for name in names:
        df = df.withColumn(name, df[name].cast(newType))
    return df
 List of continuous features
defined_cols = ['age', 'fnlwgt','capital-gain', 'educational-num', 'capital-loss', 'hours-per-week']
 Convert the type
df_string1 = convertColumn(df_string, defined_cols, FloatType())
 Check the dataset
df_string1.printSchema()
df.select('age','fnlwgt').show(5)
df.groupBy("education").count().sort("count",ascending=True).show()
df.describe().show()
df.describe('capital-gain').show()
Dropping Columns
df.drop('educational-num').columns
a=df.drop('educational-num').columns
print(a)
count the number of people above 40 year old
b=df.filter(df.age > 40).count()
print(b)
 group data by group and compute statistical operations like the mean.
 df.groupby('marital-status').agg({'capital-gain': 'mean'}).show()
 df.groupby('marital-status').orderby('native-country')
 
 c=df.filter(df['native-country'] == 'Holand-Netherlands').count()

 print(c)

 df.groupby('native-country').agg({'native-country': 'count'}).\
     sort(("count(native-country)")).show()
 df_remove = df.filter(df['native-country'] !=	'Holand-Netherlands')
 df_remove.show()
 df.show()
 d=df.replace(0,1)
 
 d.show()
  for x in df.columns:
      df3=df.withColumn(x,F.when(F.col(x))== 0, F.lit(None).otherwise(F.col(x)))
      df3.show()
 import pyspark
 from pyspark.sql import SparkSession
 from pyspark.sql.window import Window
 from pyspark.sql.functions import *

 spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

 simpleData = (("James", "Sales", 3000), \
               ("Michael", "Sales", 4600), \
               ("Robert", "Sales", 4100), \
               ("Maria", "Finance", 3000), \
               ("James", "Sales", 3000), \
               ("Scott", "Finance", 3300), \
               ("Jen", "Finance", 3900), \
               ("Jeff", "Marketing", 3000), \
               ("Kumar", "Marketing", 2000), \
               ("Saif", "Sales", 4100) \
               )

 columns = ["employee_name", "department", "salary"]
 df = spark.createDataFrame(data = simpleData, schema = columns)
 df.printSchema()
  printing the schema
 df.show(truncate=False)
 windowSpec  = Window.partitionBy("department").orderBy("salary")
 Printing the row_number
 df.withColumn("row_number",row_number().over(windowSpec)) \
     .show(truncate=False)
 printing the rank
 df.withColumn("rank",rank().over(windowSpec)) \
     .show()
 printing the dense rank
 df.withColumn("dense_rank",dense_rank().over(windowSpec)) \
     .show()
  printing the lag

 df.withColumn("lag",lag("salary",2).over(windowSpec)) \
       .show()
  printing the lead

 df.withColumn("lead",lead("salary",2).over(windowSpec)) \
     .show()
pattern
 s='python'
 p=[s[:i]for i in range(len(s)+1)]
 print(*p+p[::-1],sep='\n')

 largest of a given element
def largest(arr,n):
    mx=arr[0]
    for i in range(1,n):
        if arr[i]>mx:
            mx=arr[i]
            print(mx)
    return mx

arr=[10,324,45,90,9808]
 n=len(arr)
ans=largest(arr,n)
print("lrgest in given arry is",ans)
--------------
#########################
import json
import logging
import re
import sys
import numpy as np
import pandas as pd
import unicodedata
import unidecode
from dateutil import parser
import datetime
from calendar import monthrange

def get_object_columns(df):
    cols = df.iloc[:, 1:].select_dtypes(include=[np.object]).columns
    print('Object Columns:' + str(cols))
    return cols

def ltrim(df):
    print('In ltrim')
    df = df.applymap(lambda x: x.lstrip() if isinstance(x, str) else x)
    print('Leading Spaces Removed :'+str(df))
    return df

def rtrim(df):
    print('In rtrim')
    df = df.applymap(lambda x: x.rstrip() if isinstance(x, str) else x)
    print('Trailing Spaces Removed :' + str(df))
    return df

def remove_accents(record):
    print('In remove_accents')
    if isinstance(record, pd.DataFrame):
        cols = get_object_columns(record)
        print(cols)
        print('Object Columns :'+str(cols))
        # Remove Accents
        record[cols] = record[cols].apply(
            lambda x: x.str.normalize("NFKD").str.encode("ascii", errors="ignore").str.decode("utf-8"))
        print("Normalized df :\n" + str(record))
        return record
    elif isinstance(record, str):
        record = unidecode.unidecode(record)
        print(record,'raja')
        return record
    elif isinstance(record, list):
        for i in range(len(record)):
            record[i] = unidecode.unidecode(record[i])
        print('Accents Removed list:' + str(record))
        return record
    elif isinstance(record, dict):
        for k, v in record.items():
            if v == 'nan' or v is None or v is np.nan:
                v = ""
            else:
                v = unidecode.unidecode(v)
        print('Accents Removed dict:' + str(record))
        return record


def remove_characters_other_than_alphabets(record):
    print('In remove_characters_other_than_alphabets')
    pat = r"[^a-zA-Z\s]+"
    if isinstance(record, pd.DataFrame):
        cols = get_object_columns(record)
        # Remove some special characters
        record[cols] = record[cols].replace(pat, "", regex=True)
        print('Special characters other than alphabets removed :' + str(record))
        return record
    elif isinstance(record, str):
        record = re.sub(pat, "", record)
        print('Special characters other than alphabets removed :' + str(record))
        return record
    elif isinstance(record, list):
        for i in range(len(record)):
            record[i] = re.sub(pat, "", record[i])
        print('Special characters other than alphabets removed :' + str(record))
        return record
    elif isinstance(record, dict):
        for k, v in record.items():
            if v == 'nan' or v is None or v is np.nan:
                v = ""
            else:
                v = re.sub(pat, "", v)
        print('Special characters other than alphabets removed dict:' + str(record))
        return record

def remove_characters_other_than_alphabets_numbers(record):
    print('In remove_characters_other_than_alphabets_numbers')
    pat = r"[^A-Za-z0-9\s]+"
    if isinstance(record, pd.DataFrame):
        cols = get_object_columns(record)
        # Remove some special characters
        record[cols] = record[cols].replace(pat, "", regex=True)
        print('Special characters other than alphabets & numbers removed :' + str(record))
        return record
    elif isinstance(record, str):
        record = re.sub(pat, "", record)
        print('Special characters other than alphabets & numbers removed :' + str(record))
        return record
    elif isinstance(record, list):
        for i in range(len(record)):
            record[i] = re.sub(pat, "", record[i])
        print('Special characters other than alphabets & numbers removed list :' + str(record))
        return record
    elif isinstance(record, dict):
        for k, v in record.items():
            if v == 'nan' or v is None or v is np.nan:
                v = ""
            else:
                v = re.sub(pat, "", v)
        print('Special characters other than alphabets & numbers removed dict:' + str(record))
        return record

def remove_email_special_chars(df):
    cols = get_object_columns(df)
    # Remove some special characters
    df[cols] = df[cols].replace(r"[^a-zA-Z0-9@.\-_]+", "", regex=True)
    print('Special Characters Removed :' + str(df))
    return df

def multiple_spaces(df):
    cols = get_object_columns(df)
    # Convert 2 or more spaces between names to 1 space
    df[cols] = df[cols].replace(r"\s{2,}", " ", regex=True)
    print('Multiple Spaces Removed :' + str(df))
    return df

def remove_honorifics(df):
    pat = "^(consul general|Consul General|marchioness|Marchioness|ambassador|Ambassador|brigadier|Brigadier|commander|\
                Commander|professor|Professor|baroness|Baroness|countess|Countess|minister|Minister|princess|Princess|admiral|\
                Admiral|brother|Brother|duchess|Duchess|general|General|justice|Justice|marquis|Marquis|senator|Senator|\
                sheriff|Sheriff|consul|Consul|deputy|Deputy|father|Father|gräfin|Gräfin|madame|Madame|prince|Prince|sister|\
                Sister|madam|Madam|ma'am|Ma'am|baron|Baron|canon|Canon|chief|Chief|count|Count|judge|Judge|madam|Madam|\
                major|Major|rabbi|Rabbi|miss|Miss|lady|Lady|lord|Lord|miss|Miss|capt|Capt|cllr|Cllr|dame|Dame|duke|Duke|earl|\
                Earl|lady|Lady|lord|Lord|prof|Prof|mrs|Mrs|sir|Sir|dr.|Dr.|mrs|Mrs|col|Col|cpl|Cpl|drs|Drs|eng|ENG|hma|HMA|ing|\
                Ing|lic|Lic|llc|Llc|mme|Mme|pvt|Pvt|sgt|Sgt|sir|Sir|mr|Mr|ms|Ms|mx|Mx|mr|Mr|ms|Ms|dr|Dr|he|HE|lt|Lt)\.?"
    # Remove above Honorifics from First Name
    df.iloc[:, 1:] = df.iloc[:, 1:].replace(pat, "", regex=True)
    print('Honorifics Removed :' + str(df))
    return df

def lowercase(record):
    print('In lowercase')
    if isinstance(record, pd.DataFrame):
        record = record.applymap(lambda s: s.lower() if type(s) == str else s)
        print('Lowercased :' + str(record))
        return record
    elif isinstance(record, str):
        record = record.lower()
        print('Lowercased :' + str(record))
        return record
    elif isinstance(record, list):
        for i in range(len(record)):
            record[i] = record[i].lower()
        print('Lowercased list:' + str(record))
        return record
    elif isinstance(record, dict):
        for k, v in record.items():
            if v == 'nan' or v is None or v is np.nan:
                v = ""
            else:
                v = v.lower()
        print('Lowercased dict:' + str(record))
        return record


def remove_nan(df):
    # Replace missing values with empty string
    df = df.replace(np.nan, "", regex=True)
    df = df.replace('nan', "", regex=True)
    print('NAN Removed :' + str(df))
    return df

def remove_email_patterns(df):
    cols = get_object_columns(df)
    # Remove Email patterns
    df[cols] = df[cols].replace(r"^[^@\s]+@[^@\s]+\.[^@\s]+$", "", regex=True)
    print('Email Patterns Removed :' + str(df))
    return df

# Valid Email Attributes
def get_valid_emails(df):
    try:
        # Get input dataframe columns
        cols = df.columns
        print("{0} validation starts".format(cols[1]))
        if len(df) == df[cols[1]].isna().sum():
            return df
        # Store in numpy array
        email_records = df.values
        # Standard Email Pattern
        # pat = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        pat = r"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)"
        for row in email_records:
            matched_list = []
            # Pass null if email value is null
            if row[1] == 'nan' or row[1] is None or row[1] is np.nan:
                row[1] = ""
            else:
                for each in row[1]:
                    res = []
                    x = {}
                    # Condition to check if email is single valued attribute
                    if type(each) == str:
                        # Decode the string to maintain ascii format
                        accent = remove_accents(each)
                        # Check for email pattern matching
                        matches = re.findall(pat, accent)
                        # print("matches of str: ", matches)
                        if matches:
                            if len(matches) >= 1:
                                # Append list of email or emails ([])
                                matched_list.extend(matches)
                    # Condition to check if email is multi valued attribute
                    if type(each) == dict:
                        for key, val in each.items():
                            if key.lower() == "email_type":
                                x[key] = val
                            if key.lower() == "email":
                                # Decode the string to maintain ascii format
                                accent = remove_accents(str(val))
                                # Check for email pattern matching
                                matches = re.findall(pat, accent)
                                # print("matches of list: ", matches)
                                if matches:
                                    emails = ",".join(matches)
                                    res.append(emails)
                                    if len(matches) >= 1:
                                        x[key] = res
                                else:
                                    x[key] = None
                        # Append emails in dict format({email_type : Primary/Secondary ,email:[list of emails]})
                        matched_list.append(x)
                row[1] = matched_list
            # Replace null if no valid emails
            if len(matched_list) == 0:
                # Eliminate email record if not matching all checks
                row[1] = None
    except Exception as e:
        print("Standardization: Email failed")
        logging.debug("Standardization: Email failed with error :" + str(e))
        print('Error :' + str(e))
    # Return Standardized Emails
    df_modified = pd.DataFrame(email_records, columns=cols)
    return df_modified

def remove_non_numeric(number):
    pat = r"(?<!^)\+|[^\d+]+"
    matches = re.sub(pat, "", number)
    return matches

# Valid Phone attributes
def get_valid_phones(df):
    try:
        # Get input dataframe columns
        cols = df.columns
        print("{0} validation starts".format(cols[1]))
        # Return value as it is if all phone number is null
        if len(df) == df[cols[1]].isna().sum():
            return df
        else:
            # Store in numpy array
            phone_records = df.values
            for row in phone_records:
                phone_list = []
                # Replace null if phone value is NAN
                if row[1] == 'nan' or row[1] is None or row[1] is np.nan:
                    row[1] = None
                else:
                    for each in row[1]:
                        if type(each) == str:
                            # Condition to check if phone number is single valued and remove all non numeric chars
                            matches = remove_non_numeric(each)
                            # Replace phone value with null if number greater than 15 or less than 7
                            if len(matches) >= 15 or len(matches) <= 7:
                                row[1] = None
                            else:
                                # Append Phone number as list
                                phone_list.append(matches)
                        # Condition to check if phone number is multi valued and remove all non numeric chars
                        if type(each) == dict:
                            x = {}
                            for key, val in each.items():
                                if key.lower() == "phone_type":
                                    x[key] = val
                                if key.lower() == "phone":
                                    # Condition to check if phone number is single valued and remove all non numeric chars
                                    matches = remove_non_numeric(str(each))
                                    if len(matches) >= 15 or len(matches) <= 7:
                                        x[key] = ""
                                    else:
                                        x[key] = matches
                            phone_list.append(x)
                    row[1] = phone_list
    except Exception as e:
        print("Standardization of Phone failed")
        logging.error("Standardization of Phone failed with error :" + str(e))
        print('Error :' + str(e))
    # Return Standardized Phone Numbers
    df_modified = pd.DataFrame(phone_records, columns=cols)
    return df_modified

def remove_abbreviations(record):
    print('In remove_abbreviations')
    #print('Abbreviations Input :' + str(input_string))
    words = {
        "Dept": "Department",
        "rd": " road",
        "pl": "place",
        "ext": "extension",
        "ln": "lane",
        "ave": "avenue",
        "hwy": "highway",
        "sta": "station",
        "st": "street",
        "str": "street",
        "ne": "north east",
        "nw": "north west",
        "se": "south east",
        "sw": "south west",
        "ltd": "limited",
    }
    # Check for honorifics without space or other non characters
    pat = re.compile(r"\b(%s)\b" % "|".join(words))
    if isinstance(record, str):
        record = pat.sub(lambda m: words.get(m.group()), record)
    elif isinstance(record, list):
        for i in range(len(record)):
            record[i] = pat.sub(lambda m: words.get(m.group()), record[i])
    elif isinstance(record, dict):
        for k, v in record.items():
            if v == 'nan' or v is None or v is np.nan:
                v = ""
            else:
                v = pat.sub(lambda m: words.get(m.group()), v)
    return record

# Valid organization names
def get_valid_orgz(df):
    try:
        print("Org validation starts")
        # Return value as it is if all org records are null
        if len(df) == df['org_name'].isna().sum():
            return df
        # Store in numpy array
        org_records = df.values
        for row in org_records:
            # Condition to check if organization name is single valued and remove accent and special chars
            # Check if org name is None
            if row[1] is None or row[1] is np.nan or row[1] == 'NAN':
                row[1] = ""
            # Convert to list of multivalued separated by '|'
            if '|' in row[1]:
                row[1] = row[1].split('|')
            if type(row[1]) == str:
                # Return null if value contains only numeric
                if row[1].isnumeric():
                    row[1] = ""
                else:
                    # Standardize the Abbreviations
                    std_org = remove_abbreviations(str(row[1]))
                    # Decode to maintain ascii format
                    std_org = remove_accents(std_org)
                    # Lowercase elements in list
                    std_org = lowercase(std_org)
                    row[1] = std_org
            # Condition to check if organization name is multi valued and remove accent and special chars
            if type(row[1]) == list:
                # Decode to maintain ascii format
                row[1] = remove_accents(row[1])
                # Lowercase elements in list
                row[1] = lowercase(row[1])
                # Replace Abbreviations
                row[1] = remove_abbreviations(row[1])
                # Remove Special Characters
                row[1] = remove_characters_other_than_alphabets_numbers(row[1])
                if (len(row[1])) >= 1:
                    orgz = []
                    for org in row[1]:
                        # Return null if value contains only numeric
                        if org.isnumeric():
                            org = ""
                        else:
                            orgz.append(org)
                    row[1] = orgz
                else:
                    row[1] = ""
    except Exception as e:
        print("Standardization of Organization failed")
        logging.error("Standardization of Organization failed with error :" + str(e))
        print('Error :' + str(e))
    # Return Standardized Phone Numbers
    df_modified = pd.DataFrame(org_records, columns=['con_id', 'org_name'])
    return df_modified

# Valid Date attributes
def get_valid_dates(df):
    try:
        # Get input dataframe columns
        cols = df.columns
        print("{0} validation starts".format(cols[1]))
        # Return value as it is if all DOB is null
        if len(df) == df[cols[1]].isna().sum():
            return df
        # Store in numpy array
        date_records = df.values
        for row in date_records:
            # Replace null if DOB value is NAN
            if row[1] == 'NAN' or row[1] == "" or row[1] is None:
                row[1] = ""
            else:
                try:
                    dt_object = datetime.datetime.strptime(row[1], "%Y-%m-%d")
                except ValueError as v:
                    logging.error("Invalid Date Input :" + str(row[1]) + " - Conversion error :" + str(v))
                    row[1] = ""
                    continue
                # Convert to standard format string (dd-mm-yyyy)
                d_std = dt_object.strftime("%d-%m-%Y")
                # print('Standard {0}:{1}'.format(cols[1], d_std))
                logging.debug('Standard {0}:{1}'.format(cols[1], d_std))
                row[1] = d_std
    except Exception as e:
        print("Standardization of {0} failed".format(cols[1]))
        logging.error("Standardization of {0} failed with error :{1}".format(cols[1],str(e)))
        print('Error :' + str(e))
    # Return Standardized DOB
    df_modified = pd.DataFrame(date_records, columns=cols)
    return df_modified

# Valid address attributes
def get_valid_addresses(df):
    try:
        # Get input dataframe columns
        cols = df.columns
        print("{0} validation function starts".format(cols[1]))
        # Return value as it is if all DOB is null
        if len(df) == df[cols[1]].isna().sum():
            return df
        # Store in numpy array
        address_records = df.values
        for row in address_records:
            print('Address Input :'+str(row))
            # Check for None or Nan values
            if row[1] is None or row[1] is np.nan or row[1] == 'NAN':
                row[1] = ""
            # Convert to list if multivalued separated by '|'
            if '|' in row[1]:
                row[1] = row[1].split('|')
            if type(row[1]) == str:
                # Decode to maintain ascii format
                row[1] = remove_accents(row[1])
                # Lowercase elements in list
                row[1] = lowercase(row[1])
                # Replace Abbreviations
                row[1] = remove_abbreviations(row[1])
                # Remove Special Characters
                row[1] = remove_characters_other_than_alphabets_numbers(row[1])
            if type(row[1]) == list:
                res = []
                for each in row[1]:
                    # Decode to maintain ascii format
                    each = remove_accents(each)
                    # Lowercase elements in list
                    each = lowercase(each)
                    # Replace Abbreviations
                    each = remove_abbreviations(each)
                    # Remove Special Characters
                    each = remove_characters_other_than_alphabets_numbers(each)
                    res.append(each)
                row[1] = res
    except Exception as e:
        print("Standardization of {0} failed".format(cols[1]))
        logging.error("Standardization of {0} failed with error :{1}".format(cols[1], str(e)))
        print('Error :' + str(e))
    # Return Standardized Personal Addresses
    df_modified = pd.DataFrame(address_records, columns=cols)
    #print('Valid {0} Addresses :\n'.format((cols[1])))
    return df_modified
###################

Assignment : word count of a sentence 


def word_count(str):
    counts = dict()
    words = str.split()
    for word in words:
        if word in counts:
            counts[word] += 1
        else:
            counts[word] = 1
	
	    return counts
        
        
new_list = list([item for item in d if d[item]>1])
print(new_list) 
-------------
import json
import time
import pandas as pd
import logging
import datetime
from standardization_functions import *

LOG_FILENAME = datetime.datetime.now().strftime(
    'D:\\Users\\rnabar\\Pycharmprojects\\pythonProject\\log\\logfile_standardization_%d-%m-%y_%H%M%S.log')
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(filename=LOG_FILENAME, level=logging.DEBUG)

def get_standardization_rules():
    #myclient = pymongo.MongoClient('mongodb://localhost:27017/')
    #mydb = myclient["guid"]
    #standardization = mydb["standardization"]
    #standardization_rules_dict = {}
    #for ruledict in standardization.find():
        #for i in ruledict['std_rules']:
            #standardization_rules_dict[i] = dict['std_rules'][i]["functions"]
    # Expected output from mongo db
    Rules = {
        'First_Name': [remove_accents, remove_honorifics, remove_email_patterns, remove_characters_other_than_alphabets,
                       multiple_spaces, lowercase, ltrim, rtrim, remove_nan],
        'Middle_Name': [remove_accents, remove_honorifics, remove_email_patterns,
                        remove_characters_other_than_alphabets, multiple_spaces, lowercase, ltrim, rtrim, remove_nan],
        'Last_Name': [remove_accents, remove_honorifics, remove_email_patterns, remove_characters_other_than_alphabets,
                      multiple_spaces, lowercase, ltrim, rtrim, remove_nan],
        'Email': [remove_nan, remove_email_special_chars, lowercase, ltrim, rtrim, get_valid_emails],
        'Phone_Number': [get_valid_phones],
        'org_name': [get_valid_orgz, lowercase, remove_characters_other_than_alphabets_numbers, multiple_spaces, ltrim,
                     rtrim],
        'Date_of_Birth': [get_valid_dates, ltrim, rtrim],
        'Personal_Address': [get_valid_addresses, lowercase, remove_characters_other_than_alphabets_numbers,
                             multiple_spaces, ltrim, rtrim],
        'Professional_Address': [get_valid_addresses, lowercase, remove_characters_other_than_alphabets_numbers,
                                 multiple_spaces, ltrim, rtrim]
        }
    return Rules


def read_json():
    input = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\input\\test_file_2.json'
    # Read Input json into dataframe
    print('Reading Input Json')
    with open(input, 'rb') as f:
        data = json.load(f, encoding="utf-8")
    print('Reading Into dataframe')
    df = pd.DataFrame(data)
    #df = pd.read_json(input)
    print('Input Json Read')
    # Sort Values Alphabetically with First Name
    df = df.sort_values(['First_Name'])
    print('Dataframe Length:'+str(len(df)))
    # Drop duplicate con id's and keep first record
    df.drop_duplicates(subset="con_id", keep="first", inplace=True)
    print('Dataframe Length after removing duplicates:' + str(len(df)))
    return df

def apply_rules(rules, df):
    # Start Time
    t1 = time.time()
    json_output = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\test_output\\test_output_2.json'
    exception_records = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\input\\test_2_exception.json'
    csv_output = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\test_output\\test_output_2_csv.csv'
    final_df = df['con_id'].to_frame()
    for atr, func in rules.items():
        logging.info("Standardization of: {0} - Begins for Input Json Data".format(atr))
        print("Standardization of: {0} - Begins for Input Json Data".format(atr))
        # Get Attribute Dataframe
        inter_df = df[['con_id', atr]]
        # Apply Configured functions to attribute
        for fn in func:
            inter_df = fn(inter_df)
        # Merge Individual Attribute Standardized Dataframes
        final_df = final_df.merge(inter_df, how="inner", on=['con_id'])

    t2 = time.time()
    print('Time taken to standardize all attributes :'+str(t2-t1))
    print('Final Dataframe :\n'+str(final_df))

    # Write Final Standardized Dataframe to files
    final_df.to_csv(csv_output, index=False)
    final_df.to_json(json_output, indent=4, orient='records')


if __name__ == '__main__':
    print('Start Execution')
    # Get Standardization functions configured for attributes
    rules = get_standardization_rules()
    # Get Input Data--frame
    df = read_json()
    # Apply rules to input dataframe
    apply_rules(rules, df)
-----------
[12:33] Rohit Nabar
import json
import time
import pandas as pd
import logging
from datetime import datetime
from standardization_functions import *

LOG_FILENAME = datetime.now().strftime(
    'D:\\Users\\rnabar\\Pycharmprojects\\pythonProject\\log\\logfile_standardization_%d-%m-%y_%H%M%S.log')
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(filename=LOG_FILENAME, level=logging.DEBUG)

def read_json():
    input = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\input\\test_file_2.json'
    json_output = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\test_output\\test_output_2.json'
    exception_records = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\input\\test_2_exception.json'
    csv_output = 'D:\\Users\\rnabar\\PycharmProjects\\pythonProject\\test_output\\test_output_2_csv.csv'
    # Read Input json into dataframe
    print('Reading Input Json')
    with open(input, 'rb') as f:
        data = json.load(f, encoding="utf-8")
    print('Reading Into dataframe')
    df = pd.DataFrame(data)
    #df = pd.read_json(input)
    print('Input Json Read')
    # Sort Values Alphabetically with First Name
    df = df.sort_values(['First_Name'])
    print('Dataframe Length:'+str(len(df)))
    # Drop duplicate con id's and keep first record
    df.drop_duplicates(subset="con_id", keep="first", inplace=True)
    print('Dataframe Length after removing duplicates:' + str(len(df)))

    # Start Time
    t1 = time.time()

    name_standardization_fn = [remove_accents, remove_honorifics, remove_email_patterns, remove_characters_other_than_alphabets, multiple_spaces, lowercase, ltrim, rtrim, remove_nan]
    modified_df = df[['con_id', "First_Name", "Middle_Name", "Last_Name"]]
    print('Input DataFrame Length:' + str(len(modified_df)))
    for fn in name_standardization_fn:
        modified_df = fn(modified_df)
    print("Record count of name is: ", len(modified_df))
    logging.info("Standardization of Name Completed:\n" + str(modified_df))
    print("Standardization of Name Completed:\n" + str(modified_df))


if __name__ == '__main__':
    print('Start Execution')
    read_json()


----------
import json
import os
import datetime
import re
import shutil
import threading
import pandas as pd
import csv
from mongodb_python_connection import connection


def validate_source_input_files(*args, **kwargs):
    """
    Function to validate the source file. Called from Main
    :param args: list of all sources which are mapped from mongoDB
    :param kwargs: None
    :return: None
    """
    # Getting the Count of thread needs to be created
    no_of_threads = len(args[0])
    print("no_of_threads required-", no_of_threads)

    new_source_thread = threading.Thread(target=thread_function, args=(args[0][0],))
    new_source_thread.start()
    # for _thread in range(0, no_of_threads):
    #     #         print(args[0][_thread])
    # Creating the Thread
    #     new_source_thread = threading.Thread(target=thread_function, args=(args[0][_thread],))
    #     new_source_thread.start()
    exit()

def getcount(file):
    """
    This Function Will return the count of records in the file
    :param file: input file
    :return: count of records present
    """
    count_of_records = 0
    if file.endswith(".csv"):
        with open(file) as file:
            reader = csv.reader(file)
            count_of_records = len(list(reader))
    elif file.endswith(".json"):
        with open(file) as file:
            jsonOBJ = json.load(file)
            print(jsonOBJ)
            count_of_records = len(jsonOBJ)
    return count_of_records

def check_file_extension(file):
    """
    This function will check for the extension of the file.
    :param file: file name
    :return: True or False
    """
    if file.endswith((".csv", ".json")):
        return True
    else:
        return False

def check_delimeter(file, DELIMITER):
    """
    This function will check for the delimiter in the csv file only
    :param file: csv file, sourcename
    :return: True or False
    """
    print(file)
    print(DELIMITER)
    with open(file, newline="") as csvfile:
        try:
            dialect = csv.Sniffer().sniff(csvfile.read(1024), delimiters=DELIMITER)
            print(dialect)
            print("Delimiter is ,")
            return True
        except:
            print("Wrong Delimiter")
            return False

def check_header(file, src):
    """
    This function will check for the header weather is is present in csv file or not
    :param file: csv file
    :param src: name of the source of which the file is getting processed
    :return: True or False
    """
    print("------------------Inside check_header Function------------")
    print("File-", file)
    print("src-", src)
    global VALIDATED_FILE_DESTINATION
    try:
        with open(file) as f:
            header = csv.Sniffer().has_header(f.read(1024))
            print(header)
            if header == True:
                csv_df = pd.read_csv(file, header="infer")
                csv_columns = csv_df.columns
                #  print("csv_columns-", csv_columns)
                #  print("len csv_columns-", len(csv_columns))
                with open(source_column_mapping) as mapping_file:
                    source_column = json.load(mapping_file)
                    required_no_of_column = len(source_column[src])
                    #  print("required_no_of_column-", required_no_of_column)
                    input_column_no = 0
                    for key, value in source_column[src].items():
                        for each_val in value:
                            if each_val in csv_columns:
                                input_column_no += 1
                    print("required_no_of_column-", required_no_of_column)
                    print("input_column_no-", input_column_no)
                    print("---------Outside For Loop---------")
                    if required_no_of_column == input_column_no:
                        print("Header is Validated")
                        return True
                    else:
                        print("Write that file to Respective Error Folder")
                        print("Send Error Mail to DSO")
    except Exception as e:
        print("error-", e)

def copy_src_to_dest(ROOT_PATH, OLD_FILENAME, NEW_FILENAME, SOURCE, DESTINATION):
    """
    This is to copy the file from one source to destination
    :param ROOT_PATH: Root path of the file
    :param OLD_FILENAME: Old filename
    :param NEW_FILENAME: New filename
    :param SOURCE: Source Name
    :param DESTINATION: Destination Folder
    :return: True or False
    """
    print("------------------Inside copy_src_to_dest Fucntion------------")
    #     s3 = boto3.resource('s3')
    NEW_DESTINATION = ROOT_PATH + "/" + DESTINATION
    OLD_SOURCE = ROOT_PATH + "/" + SOURCE + "/" + OLD_FILENAME
    print("NEW_DESTINATION-", NEW_DESTINATION)
    print("OLD_SOURCE-", OLD_SOURCE)

#  s3.Object(NEW_DESTINATION, NEW_FILENAME).copy_from(CopySource = OLD_SOURCE)
#  s3.Object('my_bucket','old_file_key').delete()

def getname(file, count_of_records, source_name):
    """
    This is file to get the New Name for the File
    :param file: Name of the Old File
    :param count_of_records: Total no of Record
    :param source_name: Name of the Souce
    :return: New File Name
    """
    print("------------------Inside getname Function- Create New Name-----------")
    source = file.split('/')[-1]
    name = source.split('.')[0]
    ext = source.split('.')[1]
    print("source-", source)
    print("name-", name)
    fileName = name + '_validated_' + str(datetime.datetime.now().strftime('%d%m%Y%H%M%S')) + "_" + str(
        count_of_records) + "." + ext
    print("filename-", fileName)
    #     copy_src_to_dest(source_name, file, fileName, "input", VALIDATED_INPUT_FILE)
    return fileName

def check_valid_name(_file, source):
    # siebel_123456789
    # pattern = source + "_" + str(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'))
    # tt = str(datetime.datetime.now().strftime('%d%m%Y%H%M%S'))
    # print(tt)

    # This list will dynamically Populated from Main Function
    # sources = ['siebel', 'gigya', 'author', 'editor']   # Global Variable source_list
    filename = _file.split(".")[0]
    input_filename_source = filename.split("_")[0]
    timestamp = filename.split("_")[1]
    print(timestamp)
    if input_filename_source not in source_list:
        print(" Invalid File Name Convention")
        return False
    else:
        print("Source Name Matched")

        # Checking if the File is Placed in Respective Source Folder Only
        if input_filename_source.lower() == source.lower():
            print(" File is Properly Placed in Respective Source Folder ")
            matched = re.match(r'\d+', timestamp)
            print("matched-", matched)
            if matched is not None:
                print("Valid Input File Name")
                return True
            else:
                return False

        else:
            print(" File is Wrongly Placed in Wrong Source Folder ")
            return False

    print("------------------Inside null_check Function------------")
    print("File-", file)
    print("src-", src)
    print("sourceId-", sourceId)

    error_file = src + "_null_value_records_"
    # Getting the Columns name on Which Null Check needs to be performed
    null_check_attr = []
    for y in source_registration_schema.find({"sourceId": sourceId}):
        for _attribute in y['mandatory-attributes']:
            if y['mandatory-attributes'][_attribute]['isNull'] == 'False':
                null_check_attr.append(_attribute)
    print(null_check_attr)

    null_record_count = 0
    extension = ''
    # If the File is csv
    if file.endswith(".csv"):
        extension = '.csv'
        input_file_df = pd.read_csv(file)
        columns = list(input_file_df.columns)
        columns.append('comment')
        with open('error.csv', 'a', newline='') as f_object:
            writer_object = csv.writer(f_object)
            writer_object.writerow(columns)
            f_object.close()
        for each in null_check_attr:
            record = input_file_df[input_file_df[each].isnull()]
            input_file_df = input_file_df[input_file_df[each].notna()]
            if len(record) > 0:
                null_record_count += 1
                record['comment'] = each + " is null"
                record.to_csv('error.csv', mode='a', index=False, header=False)

    # If the File is json
    if file.endswith(".json"):
        extension = '.json'
        input_file_df = pd.read_json(file)
        print(input_file_df)
        columns = list(input_file_df.columns)
        columns.append('comment')
        with open('error.csv', 'a', newline='') as f_object:
            writer_object = csv.writer(f_object)
            writer_object.writerow(columns)
            f_object.close()
        for each in null_check_attr:
            record = input_file_df[input_file_df[each] == '']
            input_file_df = input_file_df[input_file_df[each] != '']
            if len(record) > 0:
                null_record_count += 1
                record['comment'] = each + " is null"
                record.to_csv('error.csv', mode='a', index=False, header=False)

    print(input_file_df)

    # Get New Name for the Existing Validated File Name
    count_of_records = len(input_file_df)
    print("count_of_records-", count_of_records)
    validated_file_name = getname(file, count_of_records, src)
    print("validated_file_name-", validated_file_name)

    # Writing the Remaining Not NULL record csv to a Validated file
    if extension == '.csv':
        input_file_df.to_csv(validated_file_name, index=False, header=True)
    else:
        input_file_df.to_json(validated_file_name)

    # Moving the File to Validated - Have to replace this function with S3 copy function
    shutil.move(validated_file_name, VALIDATED_FILE_DESTINATION)
    print("Moved to Validated folder")

    # Moving the Original Input File to Archive
    shutil.move(file, ARCHIVE_FILE_DESTINATION)
    print("Moved to Archive folder")

    # Copy the Error File to Error Folder and Send Mail
    error_file = error_file + str(null_record_count) + str(datetime.datetime.now().strftime('%d%m%Y%H%M%S')) + extension
    print("error_file-", error_file)
    os.rename("error.csv", error_file)
    shutil.move(error_file, ERROR_FILE_DESTINATION)
    print("Moved to Error folder")

    # Send Mail
    exit()
    return 1
def thread_function(source_name):
    """
    Function Which Will be executed by Each Thread
    :param source_name:
    :return:
    """
    print("------------------Inside Thread Function------------")
    print("source_name-", source_name)

    src = source_name.split("/")[-1]
    print("src-", src)  # siebel

    global VALIDATED_FILE_DESTINATION, ERROR_FILE_DESTINATION, INPUT, ARCHIVE_FILE_DESTINATION, sourceId, DELIMITER

    ###########################Get All requied folders from MongoDB##############
    for x in source_registration_schema.find({"sourceName": src}):
        sourceId = x['sourceId']

    for y in source_registration_schema.find({"sourceId": sourceId}):
        print(y)
        INPUT = source_name + "/" + y['folder-structure']['input']
        VALIDATED_FILE_DESTINATION = source_name + "/" + y['folder-structure']['validated']
        ERROR_FILE_DESTINATION = source_name + "/" + y['folder-structure']['error']
        ARCHIVE_FILE_DESTINATION = source_name + "/" + y['folder-structure']['archive']
        DELIMITER = y['delimeter']
        sourceId = y['sourceId']

    print(INPUT)
    print(VALIDATED_FILE_DESTINATION)
    print(ERROR_FILE_DESTINATION)
    print(ARCHIVE_FILE_DESTINATION)

    source = INPUT
    print("Input source-", source)

    # File Existence Check
    if len(os.listdir(source)) > 0:
        # no_of_files  = len(os.listdir(source))
        all_files = os.listdir(source)
        print("No of Files-", all_files)
        for _file in all_files:
            filename = source + _file
            print("Inside For Loop-", filename)
            # filesize = (os.stat(filename).st_size == 0)
            # print(filesize)
            if os.stat(filename).st_size != 0:
                print(" File is not Empty i.e greater than 0 bytes ")

                if check_file_extension(_file):
                    print(" Valid Extension for ", source, "-", _file)
                    is_name_valid = check_valid_name(_file, src)
                    if is_name_valid:
                        print(" Valid File Format ")
                        if filename.endswith(".json"):
                            # Getting the Count of Records/ Objects
                            count_of_records = getcount(filename)
                            if count_of_records > 0:
                                print(" File Contain Records ")
                                # Check for Null Record Before Sending to Standardization & Also Before Creating
                                # A single JSON File
                                

                            else:
                                print("No record Present - Write that file to ", ERROR_FILE_DESTINATION)
                                # Moving the File to Error - Have to replace this function with S3 copy function
                                shutil.move(filename, ERROR_FILE_DESTINATION)
                                # print("Send Error Mail to DSO for Invalid Header")

                        else:

                            # Validating the Header for csv file
                            valid_header = check_header(filename, src)
                            if valid_header:
                                print(" Valid Header for ", source, "-", _file)

                                # Validating the delimiter of the csv file
                                valid_delimeter = check_delimeter(filename, DELIMITER)

                                if not valid_delimeter:
                                    print("Invalid Delimeter for ", source, "-", _file)
                                    shutil.move(filename, ERROR_FILE_DESTINATION)

                                    print("Invalid Delimeter - Write that file to ", ERROR_FILE_DESTINATION)
                                    # print("Send Error Mail to DSO for Invalid Delimeter")
                                else:
                                    print("Valid Delimeter for ", source, "-", _file)
                                    count_of_records = getcount(filename)
                                    
                            else:
                                print("Invalid Header - Write that file to ", ERROR_FILE_DESTINATION)
                                # Moving the File to Error - Have to replace this function with S3 copy function
                                shutil.move(filename, ERROR_FILE_DESTINATION)
                                # print("Send Error Mail to DSO for Invalid Header")

                    else:
                        print("Invalid FileName Format or File is Wronlgy Placed in worng source Folder"
                              " - Write that file to Respective Error Folder")
                        # Moving the File to Error - Have to replace this function with S3 copy function
                        shutil.move(filename, ERROR_FILE_DESTINATION)
                        # print("Send Error Mail to DSO for Invalid Extension")
                else:
                    print("Invalid Extension - Write that file to Respective Error Folder")
                    # Moving the File to Error - Have to replace this function with S3 copy function
                    shutil.move(filename, ERROR_FILE_DESTINATION)
                    # print("Send Error Mail to DSO for Invalid Extension")
            else:
                print("Empty File - Write that file to Respective Error Folder")
                # Moving the File to Error - Have to replace this function with S3 copy function
                shutil.move(filename, ERROR_FILE_DESTINATION)
                # print("Send Error Mail to DSO for Invalid Extension")
    else:
        print("No File Present - Send Error Mail to Admin")

def main():
    #     ROOT_PATH = "ieee-source-bucket"
    global ROOT_PATH, VALIDATED_INPUT_FILE, source_column_mapping

    ROOT_PATH = "s3://ieee-source-bucket"
    VALIDATED_INPUT_FILE = "validated-input-file"
    source_column_mapping = "source_column_mapping.json"

    ####################################### Below Values are Mapped from  MongoDB ###################
    # siebel = ROOT_PATH + "/siebel"
    # author = ROOT_PATH + "/author"
    # reviewer = ROOT_PATH + "/reviewer"
    # editor = ROOT_PATH + "/editor"

    ########################### Getting Document Connection from Mongo DB############################
    global source_registration_schema, guid_collection, golden_record_collection, sourceId, source_list
    document = connection.person
    guid_collection = document.guid_repository
    golden_record_collection = document.golden_record
    source_registration_schema = document.source_registration
    domain_registration_schema = document.domain_master

    for item in domain_registration_schema.find({"doaminId": "p1111"}):
        ROOT_PATH = item['s3Bucket']

    source_list = []
    for x in source_registration_schema.find({"doaminId": "p1111"}):
        source_list.append(ROOT_PATH + "/" + x['sourceName'])

    print(source_list)
    validate_source_input_files(source_list)


if __name__ == '__main__':
    main()
-----------
import json
import time
import pytz
from pytz import timezone
from pandas import json_normalize
import requests
import datetime
from datetime import timedelta
from dateutil.tz import tzutc, UTC
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
from pandas import DataFrame
import numpy as np
from io import BytesIO
from json import dumps
from flatten_json import flatten
from pandas.io.json import json_normalize
from datetime import datetime as dt
url = "https://results.us.securityeducation.com/api/reporting/v0.1.0/phishing"
my_headers = {'x-apikey-token' : 'YV5iYWWji13z25LX/ZAQOwmHKmHqdpQk8pQzIcSQ5hGl3cpog'}
file = [line.strip() for line in url]
def reshape(r):
    dt=r["data"]
    at = dt[0]["attributes"]
    res = {
        "Type": dt[0]['type'],
        "id":dt[0]["id"],
        "UserFirstname":at['userfirstname'],
        "userLastname":at['userlastname'],
        "useremailaddress":at['useremailaddress'],
        "useractiveflag":at['useractiveflag'],
        "userdeleteddate":at['userdeleteddate'],
        "senttimestamp":at['senttimestamp'],
        "eventtimestamp":at['eventtimestamp'],
        "eventtype":at['eventtype'],
        "campaignname":at['campaignname'],
        "autoenrollment":at['autoenrollment'],
        "campaignstartdate":at['campaignstartdate'],
        "campaignenddate":at['campaignenddate'],
        "campaigntype":at['campaigntype'],
        "campaignstatus":at['campaignstatus'],
        "templatename":at['templatename'],
        "templatesubject":at['templatesubject'],
        "assessmentisarchived":at['assessmentisarchived'],
        "usertags":at['usertags'],
        "sso_id":at['sso_id']
    }
    return res
#data = json.load(json_file)
#for url  in file:
r = requests.get(url,headers=my_headers)
responses = []

responses.append(reshape(r.json()))
serialized = []
for r in responses:
    serialized.append(json.dumps(r))
    jsonlines_doc = "\n".join(serialized)
    
def lambda_handler(event, context):
    # TODO implement
    return {
    df = pd.read_json(jsonlines_doc, lines=True)
    print(df)
        'statusCode': 200,
        'body': json.dumps('Dataframe Created!')
    }


#data = json.load(a)
#print(d['data'][0]['type'])

#print(responses)



-----------------------------

import urllib.request
from pyspark import SparkContext, SparkConf
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql import functions as sf
import os
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-avro_2.11:4.0.0  pyspark-shell'
def main():
    sc = SparkContext(master="local", appName="test")
    sc.setLogLevel("Error")
    spark = SparkSession.builder.getOrCreate()
    url = "https://randomuser.me/api/0.8/?results=10"
    response = urllib.request.urlopen(url)
    datadecode = response.read().decode('utf-8')
    rdd = sc.parallelize([datadecode])
    rdd.foreach(print)
    results = spark.read.json(rdd)
    results.printSchema()
    results1 = results.withColumn("results", explode(sf.col("results"))) \
        .select("results.user.gender", "results.user.name.*", "results.user.location.*",
                "results.user.email", "results.user.username", "results.user.password",
                "results.user.salt", "results.user.md5", "results.user.sha1",
                "results.user.sha256", "results.user.registered",
                "results.user.dob", "results.user.phone", "results.user.cell",
                "results.user.picture.*", "nationality", "seed", "version") \
        .withColumn("username", regexp_replace(sf.col("username"), '[0-9]', ''))

    results1.show()
    results1.printSchema()

    results1.write.format("com.databricks.spark.avro") \
                        .mode("overwrite") \
                        .option("header", "true").option("inferSchema", "true") \
                        .save("C:/Users/Rajaraman/Desktop/softwares -spark/dataspark/results_avro")
    results1.write.format("json") \
                        .mode("overwrite") \
                        .save("C:/Users/Rajaraman/Desktop/softwares -spark/dataspark/results_json")


if __name__ == '__main__':
    main()
---------------
import pandas as pd
from datetime import datetime
import csv, ast
import pymongo

myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb = myclient["guid"]

def ML_output_collection(ml_output_path):

    data = open(ml_output_path)
    my_list = []
    GUID_val = 000
    for row in csv.DictReader(data, delimiter="|", quoting=csv.QUOTE_NONE):
        pairs = [pair.split(",", 1) for pair in ast.literal_eval(row["Pairs"])]
        row["Pairs"] = [
            {f"Pair{x}": key, "matchscore": int(val)}
            for x, (key, val) in enumerate(pairs, 1)
        ]
        row["SetRecordIDs"] = row["SetRecordIDs"].split(",")
        da = datetime.now().strftime("%Y-%m-%d_%I:%M:%S_%p")
        row["Date"] = da.split("_")[0]
        row["Curated"] = False
        row = dict(row)

        my_list.append(row)
        GUID_val += 1
        setGUID = "GUID" + str(GUID_val)
        row["Set_Id"] = setGUID
        setRecord = row["SetRecordIDs"]
        SetMatchScore = row["SetMatchScore"]

        df_merge = create_tempGUID(setRecord, setGUID,SetMatchScore)
    print("ML_output_json",my_list)
    ml_output_collection_name = mydb["ML_output"]
    DBInsert(ml_output_collection_name, my_list)
    return df_merge

df_merge = pd.read_json("./Input_File/merge.json")
df_merge["temp_GUID"] = ""
df_merge["SetMatchScore"]=""

def create_tempGUID(SetRecordIDs,temp_GUID,SetMatchScore):

    for index,row in df_merge.iterrows():
        if str(row["record_id"]) in SetRecordIDs:
            df_merge["temp_GUID"][index]  = temp_GUID
            df_merge["SetMatchScore"][index] = SetMatchScore
    return df_merge

def search_matching_records(df_mloutput):

    GUID_val = 000
    for index,row in df_mloutput.iterrows():
        GUID_val += 1
        setGUID = "GUID"+str(GUID_val)
        setRecord = row["SetRecordIDs"]
        df_merge = create_tempGUID(setRecord,setGUID)

    return df_merge

def DBInsert(mycol,mylist):
    x = mycol.delete_many({})
    if isinstance(mylist,list):
        mycol.insert_many(mylist)
    else:
        mycol.insert_one(mylist)

def populate_guid_repo(guid_repo,mydict):
    DBInsert(guid_repo, mydict)

def populate_steward_collection(mydict):
    steward_collection_name = mydb["steward"]
    DBInsert(steward_collection_name,mydict)

def MLthreshold_match(df_merge):

    domain_master_collection_name = mydb["domain_master_1"]
    new_GUID = ""
    for x in domain_master_collection_name.find():
        steward_list = []
        guid_repo_list = []
        Similar_Match = x["no_match_threshold"]
        Identical_Match = x["identical_match_threshold"]
        TEMP_GUID = ""
        temp_count = 0

        guid_collection = mydb["guid_repo"]
        x = guid_collection.insert_one(
            {"GUID": "1234", "domain": "person", "Source": "Siebel", "Primary_Key": "S123",
             "Fname": "John", "Lname": "Doe", "address": [
                {"address_type": "Primary", "address_line_1": "4D Ag Lavras st", "city": "Kifissia",
                 "postal_code": "14561", "country": "Greece"},
                {"address_type": "Work", "address_line_1": "4DD Ag Lavras st", "city": "KKifissia",
                 "postal_code": "141561", "country": "GGreece"}], "Phone": ["123-456-7809", "234-456-6789"],
             "Email": [{"type": "Primary", "emailadd": "John.doe@abc.com"},
                       {"type": "alternate", "emailadd": ["j.doe@abc.com", "john.d@xyz.com"]}],
             "degree": "PHD", "isMember": "Y", "okToCall": "N", "Publication": 3, "Consolidation_Ind": 4})
        for index,row in df_merge.iterrows():
            SetMatchScore = row["SetMatchScore"]
            if SetMatchScore != "":
                SetMatchScore = int(row["SetMatchScore"])
                if SetMatchScore > Identical_Match:
                    temp_dict = row.to_dict()
                    guid_collection = mydb["guid_repo"]
                    if temp_count == 0:
                        max_GUID_record = guid_collection.find({}, {"GUID": 1, "_id": 0}).sort([("GUID", -1)]).limit(1)
                        new_GUID = int(max_GUID_record[0]['GUID']) + 1
                        TEMP_GUID = row["temp_GUID"]
                        temp_dict["GUID"] = new_GUID
                        temp_count = temp_count + 1
                    else:
                        if TEMP_GUID == row["temp_GUID"]:
                            temp_dict["GUID"] = new_GUID
                        else:
                            max_GUID_record = new_GUID
                            new_GUID = max_GUID_record + 1
                            TEMP_GUID = row["temp_GUID"]
                            temp_dict["GUID"] = new_GUID
                    temp_dict["Consolidation_Ind"] = 4
                    temp_dict["domain"]= "person"
                    guid_repo_list.append(temp_dict)
                elif Identical_Match > SetMatchScore > Similar_Match:
                    steward_dict = row.to_dict()
                    steward_dict["Consolidation_Ind"] = 2
                    steward_list.append(steward_dict)
                elif SetMatchScore < Similar_Match:
                    max_GUID_record = new_GUID
                    new_GUID = max_GUID_record + 1
                    temp_dict["GUID"] = new_GUID
                    temp_dict["Consolidation_Ind"] = 4
                    temp_dict["domain"] = "person"
                    populate_guid_repo(guid_collection, temp_dict)
                    #print("Below 35 - No Match",new_GUID)
        populate_guid_repo(guid_collection,guid_repo_list)
        populate_steward_collection(steward_list)

def main():

    ml_output_path = "./Input_File/ML_Input.csv"
    df_merge = ML_output_collection(ml_output_path)
    MLthreshold_match(df_merge)
    df_merge.to_json("./Output_File/temp_guid.json",orient="records")

if __name__ == "__main__":
    main()
------------











    